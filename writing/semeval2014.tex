%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx}
\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}


\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\sam}[1]{\textcolor{blue}{[#1 -SMT]}}
\newcommand{\jdcomment}[1]{\textcolor{NavyBlue}{[#1 -JDD]}}

% turn off for submission, but on for a more tech report-y version.
\newcommand{\codenote}[1]{\textcolor{PineGreen}{[Code: \emph{#1}]}}
\newcommand{\logitedge}{\textsc{LogitEdge}}
\newcommand{\noedge}{\textsc{NoEdge}}


\title{Broad-Coverage Semantic Dependency Parsing with Rich Edge-Factored
Features}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present an edge-factored statistical model for parsing to the three semantic
dependency formalisms in SemEval 2014 Shared Task 8: Broad-Coverage Semantic
Dependency Parsing.

\end{abstract}



\section{Introduction}

We present a system that produces semantic dependency parses by extracting
features for each potential dependency arc and learning a statistical model to
discriminate between good arcs and bad.
The organizers of SemEval 2014 Shared Task 8: Broad-Coverage Semantic Dependency
Parsing \sam{cite} have provided a corpus of English language newswire
annotated with three different semantic dependency formalisms
(\S\ref{s:formalisms}).
We test our system for each of the three formalisms using held out data
from the provided corpus.

Building shallow semantic analyzers\footnote{By shallow semantic analysis we
mean analyzing the semantic relationships between pairs of words or concepts in
a sentence.
This is in contrast to a deeper semantic analysis in which one produces a full
lambda-calculus logical form, or other representation which can be grounded in a
model of the world or from which further inferences can be made.
}
that can analyze broad-domain text quickly
and accurately remains an open challenge.
\begin{itemize}
\item applications of semantic parsing: information extraction, knowledge base
population, question answering, etc. \sam{get good cites}
\end{itemize}
While semantic parsing remains unsolved, syntactic parsing in general, and
dependency parsing in particular has gotten very far towards these goals.
%Syntactic parsing has greatly benefited from the existence of a large corpus of
%annotations with high inter-annotator agreement: the Penn Treebank
%\cite{marcus_building_1993}.
By representing semantic relationships between words as labeled
directed edges, we can reuse methodologies developed for syntactic dependency
parsing.
These methods are noted for their fast inference.


%\sam{something about how we're inspired by the success
%dependency parsing has had, because of PTB, and because loosening up the
% representation allows fast inference.}

%Dependency parsing is a popular form of syntactic processing, as it
%is generally faster than constituency parsing, while still capturing much of
%what is needed for downstream applications.
% allows for a simpler representation

But while syntactic dependency parsers capture relationships between words in a
sentence, the relationships they capture are not semantic.
For instance, in ``The bread baked in the oven.'' and ``I baked the bread in the
ven.'' ``the bread'' bears the same semantic relationship to ``baked'' but is
realized in a different syntactic position.
These types of distinctions become important in downstream applications.
%Even shallow semantic analysis can be useful in downstream applications.


\sam{cite}.

\sam{This paper is organized as follows\ldots}



\section{Formalisms} \label{s:formalisms}

\begin{itemize}
	\item Quick overview of three formalisms, emphasizing differences between them, consequences for parsing
	\item DM:
	\begin{itemize}
	  \item DM is automatically derived from LinGO English Resource Grammar (ERG)
	  annotations in DeepBank \cite{flickinger_deepbank_2012}.
	\end{itemize}
	\item PAS:
	\begin{itemize}
	  \item PAS is automatically derived from the Enju HPSG treebank using the
	  conversion rules of \newcite{miyao_corpus_oriented_2004}.
	\end{itemize}
	\item PCEDT:
	\begin{itemize}
	  \item PCEDT is automatically derived from the tectogrammatic layer of
	  the Prague Czech--English Dependency Treebank \cite{hajic_building_1998}.
	\end{itemize}
	\item All are represented as labeled directed graphs, with words as vertices.
	\item All have ``top'' annotations 
	\item Stats about \% multiple roots, \% multiple tops, \% tree, \% acyclic
	\item Examples? 
\end{itemize}


\section{Models}

We treat the problem as a three-stage pipeline.
The first stage prunes words by predicting whether a word is a \emph{singleton}
or not--whether it has any incoming or outgoing edges at all (\S\ref{s:singleton_model}).
If a word is predicted to be a singleton, then it no longer needs to be
considered in later stages.
The second stage jointly predicts whether an edge is
present, and if so, what its label is (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
``Top'' annotations roughly correspond to the semantic focus of a
sentence, but there is no constraint that the ``top'' must be a root of the
semantic dependency graph.
Formalisms sometimes annotate more than one ``top'' per sentence, but we
found that we achieve the best performance on all formalisms by predicting only
the one best-scoring ``top'' under the model.


\subsection{Singleton Classification} \label{s:singleton_model}

We call a node in a graph without any parents or children a \emph{singleton}.
For singleton prediction, we train a simple token-level logistic regression
classifier.
We train a model for each of the three formalisms, as each of the three
formalisms follow different conventions determining whether a token would be
included in the final graph for a sentence.
The PAS formalism, for example, includes almost all tokens that are not
punctuation or a determiner, and our singleton model achieves over 99\%
accuracy.
\jdcomment{I need to find the accuracy for the other two formalisms. Will
include this in our Results section.}

For each token $t$, the model considers the following features:
\begin{itemize}
\item the token $t$ itself
\item the lemma of $t$
\item the part of speech tag of $t$
\end{itemize}



\subsection{Edge Prediction} \label{s:edge_model}

In the second stage of the pipeline, we predict the set of labeled directed
edges in the graph.
We use the same set of edge-factored features (\S\ref{s:features}) in two
different models.
For the DM and PCEDT formalisms, we use an edge-independent multiclass logistic
regression model (\logitedge, \S\ref{s:logitedge}).
For the PAS formalism, we use a structured SVM 
\cite{taskar_max_2003,tsochantaridis_support_2004} that enforces a
\emph{determinism} constraint -- a word may not have two outgoing edges with the
same label (\S\ref{s:graphparser}).
\sam{explain why we use different models for different formalisms}

Both models consider only token index pairs $(i, j)$ where %that are within 10
% tokens of each other (i.e.~
$|i-j| \leq 10$, $i \ne j$, and both $i$ and
$j$ have been predicted to be non-singletons by the first stage.
In other words, edges with more than 9 tokens between its two endpoints are not
considered.
Although this prunes some gold edges, among the formalisms,
95\%-97\% of all gold edges are between tokens of distance 10 or less.
We found that pruning long edges increased accuracy overall, in addition to
speeding up training and prediction.
Both directions $i \rightarrow j$ and $j \rightarrow
i$ are considered between every pair.


\subsubsection{\logitedge\ parser} \label{s:logitedge}


\codenote{LRParser.java}

%\noindent
The \logitedge\ model treats every ordered pair of token indices $(i, j)$ as a
multiclass logistic regression with $K+1$ possible outputs:
the formalism's original $K$ edge labels plus the additional label \noedge,
which indicates that no edge exists from $i$ to $j$.
Let $L$ be the set of possible edge labels.

In our implementation, we separate features into two classes:
those which can be extracted for any one word, $\bm{f}^{(\text{node})}(i)$;
and those which look at both the parent and the child,
$\bm{f}^{(\text{edge})}(i, j)$.
These features are described in \S\ref{s:features}.
The final binary feature vector is made by conjoining the output label with
each:
\begin{align*} 
\bm{f}(i, j, \ell) =
\{ \ell \} \otimes \left( &
	\{ \text{``bias''} \} \\
	& \cup \bm{f}^{(\text{edge})}(i, j) \\
	& \cup \{ \text{``parent''} \} \otimes \bm{f}^{(\text{node})}(i) \\
	& \cup \{ \text{``child''} \} \otimes \bm{f}^{(\text{node})}(j)
\right)) \\
\end{align*}
\noindent
where  $\otimes$ represents the Cartesian product.
%Feature extractors are defined to extract
%features both for individual tokens ($f^{(\text{node})}(i)$), and also for
% pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output
%labels.
For candidate parent index $i$ and child index $j$, the model defines a
distribution over $y_{i,j} \in L$, parametrized by the weight vector $\bm\phi$,

\begin{equation}
  Pr(y_{i,j}=\ell; \bm\phi) & = \frac{
  	e^{\bm\phi^\top \bm{f}(i, j, \ell)}
  } {
  	\sum_{\ell^\prime \in L} {
  		e^{\bm\phi^\top \bm{f}(i, j, \ell^\prime)}
  	}
  }
\end{equation}

\noindent
%In this notation, an $f$ function outputs a real-valued vector, defined as the
% concatenation of features from all the feature classes (\S\ref{s:features}).
$\bm\phi$ is learned by minimizing total negative log-likelihood of the above
(with weighting; see below), plus $\ell_2$ regularization.
Adagrad \cite{duchi_adaptive_2011} is used for optimization.
It seemed to optimize faster than L-BFGS\sam{cite}, at least for earlier
iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

In the FFF formalism, the training set (\S\ref{s:datasplits})
contains NNN candidate edges (pairs of tokens with length between 1 and 10 inclusive),
and NNN actual (non-null) edges.  $F_1$ performance was improved by
downweighting $\noedge$ examples through a weighted log-likelihood objective,
$\sum_{i,j} \sum_k w_k \log P(y_{i,j}=\ell; \bm\phi)$, with $w_{\noedge}=0.3$
and $1$ otherwise.
(The weight was chosen by gridsearch on a development set; 
PAS used a weight of $0.4$ while the others were $0.3$, though the differences at this granularity were small.)

% Besides the edge logistic regression system, there were both pre- and post-processing steps.

% \textbf{Preprocessing:}
% \bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

% \textbf{Decoding and postprocessing:}
\textbf{Decoding:}
\codenote{decodeEdgeProbsToGraph(), MyGraph.java}

To predict a graph structure at test-time for a new sentence,
the most likely edge label is predicted for every candidate $(i, j)$ pair of
tokens that has not been pruned by an earlier stage.
%There were several post-processing steps.
We enforce only one graph constraint, which is that there cannot be
an edge in both directions between any pair of words.
If an edge is predicted for both directions for a single $(i, j)$
pair, only the edge with the higher score is chosen.
% If the gold standard has an edge in the direction $i \rightarrow j$, the
% direction $j \rightarrow i$ is considered a \noedge.
There are no such bidirectional edges in the training data, and enforcing this
restriction improved accuracy on the development set.%, though it is only a
% primitive constraint on the graph decoding.



\subsubsection{Graph-based Structured SVM Parser} \label{s:graphparser}

To predict edges for the PAS formalism, we use a structured SVM model
\sam{cite}.
The model uses the same set of features as \logitedge, but optimizes a
max-margin objective during training, and enforces an additional constraint: no
node may have more than one outgoing edge with the same label.

\sam{TK from jflan}


\subsubsection{Edge Features}

\label{s:features}
These features were computed over an edge $E$ with parent token $S$ and child
token $T$.  For each of those listed here, we have an indicator feature for each value it takes on.

\begin{itemize}
\item Basic
\begin{itemize}
\item Lemmas of $S$ and $T$.
\item Part of speech tags of $S$ and $T$.
\item Token strings of $S$ and $T$.
\item 1 if Index($S$) $\le$ Index($T$), 0 otherwise.
\end{itemize}
\item LinearOrder
\begin{itemize}
\item Index($S$) - Index($T$).
\end{itemize}
\item CoarseDependency
\begin{itemize}
\item 1 if $S$ is the parent of $T$ in syntactic dependency parse, 0 otherwise.
\item Distance between $S$ and $T$ in syntactic dependency parse.
\end{itemize}
\item LinearContext
\begin{itemize}
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($S$)+1, Index($T$)-1, Index($T$), Index($T$)+1.
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($T$)-1, Index($T$).
\item Concatenated PoS tags of tokens at Index($S$), Index($S$)+1, Index($T$), Index($T$)+1.
\end{itemize}
\item DependencyPath
\begin{itemize}
\item The token string concatenated to the labeled path through the syntactic dependency tree from $S$ to $T$.
\end{itemize}
\item SubcatSequence
\begin{itemize}
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the label of the ark to the child. If $c$ is $T$, append a ``+''.
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the PoS tag of $c$ and the label of the ark to the child. If $c$ is $T$, append a ``+''.
\end{itemize}
\item UnlabeledDep
\begin{itemize}
\item The unlabeled path through the syntactic dependency tree from $S$ to $S$. 
\item The unlabeled path through the syntactic dependency tree from $S$ to $T$, annotated with whether the each step through the tree was to the right or left in the sentence.
\end{itemize}
\item Brown clusters
\begin{itemize}
\item \jdcomment{The word on the street is that these weren't used. Check on that, then consider moving to ``didn't work'' section.}
\end{itemize}
\end{itemize}

\subsubsection{Feature Hashing}

During our experiments, the number of instantiated features ranged into the tens
of millions.
As a memory optimization, we implemented feature hashing
\cite{weinberger_feature_2009}.
Let $F$ be the space of all feature names, and let $h$ be a hash function
$h : F \rightarrow \{0, \ldots, k-1\}$ with $k < |F|$.
Instead of storing our model parameters as a
hashmap %from \texttt{feature\_name} to \texttt{feature\_value},
or array $\bm\phi$ of size $|F|$, we instead store them in an array
$h(\bm\phi)$ of size $k$, where the $i^{th}$ component of $h(\bm\phi)$ is
defined to be
\[
h(\bm\phi)_i = \sum_{\substack{\text{feat} \in F \\\text{ s.t. }
h(\text{feat})=i}}{\phi_{\text{feat}}}
\]
We similarly store our feature vectors as $h(\bm{f}) \in 2^k$ instead of
$\bm{f} \in 2^{|F|}$.
Note that the dot product is preserved,
$h(\bm\phi)^\top h(\bm{f}) = \bm\phi^\top \bm{f}$.
\sam{mention fact that dot products are approximately
preserved with high probability.
mention that should use the sign trick to get an unbiased approximation, but
we don't happen to.}
By never explicitly storing the full set of feature names, we can use
tens of millions of features while keeping a fixed memory overhead.
The downside is that whenever there is a collision, and two features hash to the
same value, our model can no longer differentiate between those two features.
In practice, we choose large enough $k$ such that performance is unaffected by
collisions.
\sam{which $k$ did we end up using? how many collisions?}


\subsection{Top Prediction} \label{s:top_model}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all nodes in the predicted graph (i.e., tokens where there is
either an inbound or outbound edge) are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top''.
This is suboptimal, since some graphs have multiple tops (and in PCEDT this is
more common);
but selection rules based on probability thresholds gave worse $F_1$
performance on the development set. \bocomment{I wonder if I documented this on
an issue/PR}


\subsubsection{Top Features}
For a given token $T$, the topness classifier used the following features:
\begin{itemize}
\item The PoS tag of $T$.
\item Index($T$).
\item Conjoined PoS tag of $T$ and Index($T$).
\item The depth of $T$ in the syntactic dependency parse. 
\end{itemize}






\section{Negative Results}

We followed a forward-selection process during feature engineering.
For each potential feature, we tested the current feature set versus the current
feature set plus the new potential feature.
If the new feature did not improve performance, we did not add it.
We list below some of the features which we tested, but did not find to improve
performance.

Note that in order to save time, we ran these feature selection experiments
on a subsample of the training data, for a reduced number of iterations.
So please regard the following with the strong caveat that our experiments were
not exhaustive, and we do not claim that these features could not be found to
help.

\subsection{Word vectors}
Word vectors have been shown to benefit a number of semantic tasks. We
incorporated features based on trained word vectors in a number of ways, though
in this task we did not find benefit from any of them.
The vectors we used \cite{wordVectors} were trained trained on the WMT-2011
dataset, using a variation on latent semantic analysis which incorporated
information from multiple languages.
The vectors contained 64 dimensions.

For our edge factored model \logitedge, when considering a potential edge, we
had a parent word $W_1$ with associated word vector $V_1$ and a child word
$W_2$ with associated word vector $V_2$.
For each method outlined below, the elements of the output vector was used as a
feature.
As an example, if we concatenated the two vectors into a third vector $V_3$ (as
we show in $f_1$), each element of $V_3$ was used as a feature in our model.

\begin{enumerate}
\item $f_1(V_1,V_2)$ = $ \left( \begin{smallmatrix} V_1\\ V_2 \end{smallmatrix} \right)$
\item $f_2(V_1,V_2)$ = $V_1 - V_2$.
\item $f_3(V_1,V_2)$ = $V_1 \cdot V_2$ (Dot product)
\item $f_4(V_1,V_2)$ = $V_1 \bigodot V_2$ (Element-wise multiplication)
\end{enumerate}
Unfortunately we saw no improvement from these features.

\subsection{Random Forests Over Word Vectors}
The word vectors added as features didn't reliably improve accuracy on any of
the formalisms.
One hypothesis as to why was that the vectors had a non-linear relationship
with the labels.
To address this, we trained a random forest in
R\footnote{\url{http://cran.r-project.org/web/packages/randomForest/randomForest.pdf}}
with the word vector features as described above, and used the hard labels
predicted by the random forests as features in the \logitedge\ parser.
Again, we saw no improvement.

\bocomment{caveat, these were not carefully done.}


\section{Evaluation}
\label{s:datasplits}

We evaluate labeled precision (LP), labeled recall (LR), labeled $F_1$ (LF), and
labeled whole-sentence match (LM) on the held out test data (see
Table~\ref{table:perf}).

% CMU:
% DM:
% LP: 0.844641
% LR: 0.834849
% LF: 0.839716
% LM: 0.087537
% 
% 
% PAS:
% LP: 0.907832
% LR: 0.885141
% LF: 0.896343
% LM: 0.260386
% 
% 
% PCEDT:
% LP: 0.768139
% LR: 0.707173
% LF: 0.736396
% LM: 0.071217

% Priberam:
% DM:
% LP: 0.902322
% LR: 0.881050
% LF: 0.891559
% LM: 0.268546
% 
% PAS:
% LP: 0.925576
% LR: 0.909676
% LF: 0.917557
% LM: 0.378338
% 
% PCEDT:
% LP: 0.801397
% LR: 0.757900
% LF: 0.779042
% LM: 0.106825


\begin{table*}
\begin{center}
\begin{tabular*}{\textwidth}%{.6\textwidth}
{@{\extracolsep{\fill}}cc|cccc}%|c|c|c|c|c|c|}
% \hline
 & System & LP & LR & LF & LM \\
\hline
\hline
\multirow{2}{*}{\textbf{DM}}
& CMU & 0.8446 & 0.8348 & 0.8397 & 0.0875 \\
& Prib & 0.9023 & 0.8811 & 0.8916 & 0.2685 \\
\hline
\multirow{2}{*}{\textbf{PAS}}
& CMU & 0.9078 & 0.8851 & 0.8963 & 0.2604 \\
& Prib & 0.9256 & 0.9097 & 0.9176 & 0.3783 \\
\hline
\multirow{2}{*}{\textbf{PCEDT}}
& CMU & 0.7681 & 0.7072 & 0.7364 & 0.0712 \\
& Prib & 0.8014 & 0.7580 & 0.7790 & 0.1068 \\
\hline
\hline
\multirow{2}{*}{\textbf{Average}}
& CMU & 0.8402 & 0.8090 & 0.8241 & 0.1397 \\
& Prib & 0.8764 & 0.8496 & 0.8627 & 0.2512 \\
\end{tabular*}
\caption{Labeled precision (LP), recall (LR), $F_1$ (LF), and
whole-sentence match (LM) on the held out test data.
We show results for our system (CMU) and the winning system (Prib) \sam{how to
cite?}}
\label{table:perf}
\end{center}
\end{table*}



\section{Future work}

\begin{itemize}
  \item Higher-order features
  \item Multitask learning
\end{itemize}


\section{Conclusion}

\nocite{flanigan-etal:ACL2014}



\section*{Acknowledgements}

\bibliographystyle{acl}
\bibliography{semeval8}




\end{document}
