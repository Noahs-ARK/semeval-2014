%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\sam}[1]{\textcolor{blue}{[#1 -SMT]}}

% turn off for submission, but on for a more tech report-y version.
\newcommand{\codenote}[1]{\textcolor{PineGreen}{[Code: \emph{#1}]}}
\newcommand{\logitedge}{\textsc{LogitEdge}}
\newcommand{\noedge}{\textsc{NoEdge}}


\title{Broad-Coverage Semantic Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a graph-based approach to semantic dependency parsing for the
three formalisms in SemEval 2014 Shared Task 8: Broad-Coverage Semantic
Dependency Parsing.
  
\end{abstract}



\section{Introduction}

We desire shallow semantic analyzers which can analyze broad-domain text quickly
and accurately.
\begin{itemize}
\item applications of semantic parsing
\end{itemize}

Syntactic parsing has greatly benefited from the existence of a large standard
corpus of annotations \sam{cite PTB}.
Dependency parsing is a popular form of syntactic processing, as it
is generally faster than constituency parsing, while still capturing much of
what is needed for downstream applications.
% allows for a simpler representation
But while syntactic parsers capture relationships between words in a sentence,
the relationships they capture are not semantic.
For instance, in ``The bread baked in the oven.'' and ``I baked the bread in the
ven.'' ``the bread'' bears the same semantic relationship to ``baked'' but is
realized in a different syntactic position.

\section{Formalisms}

\begin{itemize}
\item Quick overview of three formalisms, emphasizing differences between them, consequences for parsing
\end{itemize}


\section{Models}

We treat the problem as a three-stage pipeline.
The first stage predicts whether a word is a \emph{singleton} or not--whether it
has any incoming or outgoing edges at all (\S\ref{s:singleton_model}).
The second stage jointly predicts whether an edge is present, and if so, what
its label is (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
``Top'' annotations roughly correspond to the focus of a
sentence.


\subsection{Singleton Prediction} \label{s:singleton_model}

We use a logistic regression model, tuned for high precision.
\sam{TK from jdodge}

\subsection{Edge Prediction} \label{s:edge_model}

In the second stage of the pipeline, we predict the set of labeled directed
edges in the graph.
We used the same set of (first-order) features
in two different models: an edge-independent multiclass %a faster-to-train
logistic regression model (\logitedge, \ref{s:logitedge}), which we use for the
DM and PCEDT formalisms, and a discriminative structured graph prediction model
(\ref{s:graphparser}) for the PAS formalism.

Both models consider only tokens pairs $(i, j)$ where %that are within 10 tokens
%of each other (i.e.~
$|i-j| \leq 10$, $i \ne j$, and both $i$ and
$j$ have been predicted to be non-singletons by the first stage.
Although this prunes a few gold edges, among the formalisms, 95\%-97\% of all
gold edges are between tokens of distance 10 or less.
Both directions $i \rightarrow j$ and $j \rightarrow i$ are considered between
every pair.


\subsubsection{\logitedge\ parser} \label{s:logitedge}


\codenote{LRParser.java}

\noindent
This model treats every pair of tokens as a multiclass logistic regression
with $K+1$ possible outputs:
either to be an edge with one of the formalism's $K$ different labels, or a null
\noedge decision.  

Feature extractors are defined to extract features both for individual tokens ($f^{(\text{node})}(i)$), and also for pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output labels.
For candidate head $i$ and child $j$, the model defines a distribution over
$y_{ij} \in \{\text{edge labels}\} \cup \{\noedge\}$, using $3$ weight matrixes
$\beta$,

\begin{align*} 
  P(y_{ij}=k; \beta) & \propto 
  \exp \big( \beta^{\text{(bias)}}_k \ \ + \\
  &
  \beta^{\text{(node as head)}}_k \cdot f^{\text{(node)}}(i)
  \ \ + \\ 
  &
  \beta^{\text{(node as child)}}_k \cdot f^{\text{(node)}}(j)
  \ \ + \\
  &
  \beta^{\text{(edge)}}_k \cdot f^{\text{(edge)}}(i,j)
    \big)
\end{align*}

\noindent
In this notation, an $f$ function outputs a real-valued vector, defined as the concatenation of features from all the feature classes (\S\ref{s:features}).
Training minimizes total negative log-likelihood of the above (with weighting; see below),
plus $\ell_2$ regularization.  Adagrad \cite{duchi_adaptive_2011} is used for optimization.
It seemed to optimize faster than L-BFGS, at least for earlier iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

In the FFF formalism, the training set (\S\ref{s:datasplits})
contains NNN candidate edges (pairs of tokens with length between 1 and 10 inclusive),
and NNN actual (non-null) edges.  $F$-score performance was improved by
downweighting $\noedge$ examples through a weighted log-likeihood objective,
$\sum_{i,j} \sum_k w_k \log P(y_{ij}=k;\beta)$, with $w_{\noedge}=0.3$ and $1$ otherwise.  
(The weight was chosen by gridsearch on a development set; 
PAS used a weight of $0.4$ while the others were $0.3$, though the differences at this granularity were small.)

% Besides the edge logistic regression system, there were both pre- and post-processing steps.

% \textbf{Preprocessing:}
% \bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

% \textbf{Decoding and postprocessing:}
\textbf{Decoding:}
\codenote{decodeEdgeProbsToGraph(), MyGraph.java}

To predict a graph structure at test-time for a new sentence,
the most likely edge label is predicted for every candidate $(i, j)$ pair of
tokens that has not been pruned by an earlier stage.
%There were several post-processing steps.
We enforce only one graph constraint, which is that there cannot be
an edge in both directions between any pair of words.
If an edge is predicted for both directions for a single $(i, j)$
pair, only the edge with the higher score is chosen.
% If the gold standard has an edge in the direction $i \rightarrow j$, the
% direction $j \rightarrow i$ is considered a \noedge.
There are no such bidirectional edges in the training data, and enforcing this
restriction improved accuracy on the development set.%, though it is only a
% primitive constraint on the graph decoding.



\subsubsection{Graph-based parser} \label{s:graphparser}

\subsection{Top Prediction} \label{s:top_model}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all nodes in the predicted graph (i.e., tokens where there is
either an inbound or outbound edge) are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top''.
This is suboptimal, since some graphs have multiple tops (and in PCEDT this is
more common);
but selection rules based on probability thresholds gave worse F-score
performance on the devset. \bocomment{I wonder if I documented this on an issue/PR}

\section{Features}

\subsection{all the stuff in our model}

\label{s:features}

\begin{itemize}
\item BasicFeatures
\item LinearOrderFeatures
\item CoarseDependencyFeatures
\item LinearContextFeatures
\item DependencyPathv1
\item SubcatSequenceFE
\item UnlabeledDepFE
\item Brown clusters
\end{itemize}



\subsection{Feature Hashing}

As a memory optimization, we implemented feature hashing
\cite{weinberger_feature_2009}.
Let $F$ be the space of all feature names.
Instead of storing our model parameters as a hashmap %from
% \texttt{feature\_name} to \texttt{feature\_value},
or array $\phi$ of size $|F|$, we instead store them in an array $\psi$ of size
$k$, with $k < |F|$.
We choose a hash function $h : F \rightarrow \{0, \ldots, k-1\}$, and
let the $i^{th}$ component of $\psi$ be 
\[
\psi_i = \sum_{\text{feat}\in F \text{ s.t. }
h(\text{feat})=i}{\phi_{\text{feat}}}
\]
By never explicitly storing the full set of feature names, we can use
tens of millions of features while keeping a fixed memory overhead.
The downside is that whenever there is a collision, and two features hash to the
same value, our model can no longer differentiate between those two features.
In practice, we choose large enough $k$ such that performance is unaffected by
collisions.

\subsection{Features for subsystem classifiers}

 - Topness classifier

 - Singleton classifier

 - Predicate-ness classifier


\subsection{all the stuff we tried but didn't work}

\bocomment{caveat, these were not carefully done.}


\section{Evaluation}
\label{s:datasplits}

\section{Future work}

\section{Conclusion}

\nocite{flanigan-etal:ACL2014}



\section*{Acknowledgements}

\bibliographystyle{acl}
\bibliography{semeval8}




\end{document}
