%
% File twocolumn.tex
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{framed}
\usepackage{subcaption}

\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}


\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\sam}[1]{\textcolor{blue}{[#1 -SMT]}}
\newcommand{\nas}[1]{\textcolor{red}{[#1 -NAS]}}
\newcommand{\jmf}[1]{\textcolor{orange}{[#1 -JMF]}}
\newcommand{\jdcomment}[1]{\textcolor{NavyBlue}{[#1 -JDD]}}

 \renewcommand{\bocomment}[1]{}
 \renewcommand{\sam}[1]{}
 \renewcommand{\nas}[1]{}
 \renewcommand{\jmf}[1]{}
 \renewcommand{\jdcomment}[1]{}


% turn off for submission, but on for a more tech report-y version.
% \newcommand{\codenote}[1]{\textcolor{PineGreen}{[#1]}}
\newcommand{\codenote}[1]{}
\newcommand{\logitedge}{\textsc{LogisticEdge}}
\newcommand{\svmedge}{\textsc{SvmEdge}}
\newcommand{\noedge}{\textsc{NoEdge}}

% saves only a little bit of space. might need tweaking to save more.
\newenvironment{itemizesquish}{\begin{list}{\labelitemi}{\setlength{\itemsep}{-0.1em}\setlength{\labelwidth}{1em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}

% \renewcommand{\baselinestretch}{0.95}



\title{CMU-ARK: Arc-Factored, Discriminative Semantic Dependency Parsing}

\author{
	Sam Thomson \quad
	Brendan O'Connor \quad
	Jeff Flanigan \quad
	David Bamman \quad  \\
	\bf{Jesse Dodge \quad
	Swabha Swayamdipta \quad
	Nathan Schneider \quad
	Chris Dyer \quad
	Noah A.~Smith} \\
  Language Technologies Institute \\
  Carnegie Mellon University \\
  Pittsburgh, PA 15213, USA \\
  {\tt\{sthomson,brenocon,jflanigan,dbamman,jessed,}\\
   \tt{swabha,nschneid,cdyer,nasmith\}@cs.cmu.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present an arc-factored statistical model for broad-coverage
semantic dependency parsing, as defined by the SemEval 2014 Shared
Task 8 on
Broad-Coverage Semantic Dependency Parsing.    Our entry in the open
track placed second in the competition.
\end{abstract}



\section{Introduction}

The task of broad coverage semantic dependency parsing aims to provide a
shallow semantic analysis of text not limited to a specific domain.
As distinct from deeper semantic analysis (e.g., parsing to a full
lambda-calculus logical form), shallow semantic parsing captures relationships
between pairs of words or concepts in a sentence, and has wide application for
information extraction, knowledge base population, and question answering (among others).
%The SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency
%Parsing \cite{oepens_broad_2014} includes three different semantic
%formalisms (\S\ref{s:formalisms}), which encourages finding features and methods
%that are broadly applicable to semantic parsing, and discourages optimization
%toward a single semantic representation.
%By diversifying the representations, we better tackle the broader task of
%semantic analysis itself.

We present here two systems that produce semantic dependency parses in the three formalisms of the SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency
Parsing \cite{oepens_broad_2014}.  These systems generate parses by extracting
features for each potential dependency arc and learning a statistical model to
discriminate between good arcs and bad;
the first treats each labeled edge decision as an independent multiclass
logistic regression (\S\ref{s:logitedge}), while the second predicts arcs
as part of a graph-based structured support vector machine (\S~\ref{s:graphparser}).
Common to both models are a rich set of features on arcs, described in
\S\ref{s:edgefeatures}.  We include a discussion of features found to
have no discernable effect, or negative effect, during development (\S\ref{s:badfeatures}).

Our system placed second in the open track of the broad-coverage semantic
parsing task (in which output from syntax parsers and other outside resources \emph{can} be used).
We present our results in \S\ref{s:evaluation}.



\section{Formalisms} \label{s:formalisms}
The dataset on which Shared Task 8 focuses is comprised of annotations of the Penn Treebank in three different semantic dependency formalisms.
%The organizers of Shared Task 8 provided a corpus of English news text
%drawn from the Penn Treebank, annotated according to three different semantic dependency formalisms.
\emph{DM} is derived from LinGO English Resource Grammar (ERG)
annotations in DeepBank \cite{flickinger_deepbank_2012}.
\emph{PAS} is derived from the Enju HPSG treebank using the
conversion rules of \newcite{miyao_corpus_oriented_2004}.
\emph{PCEDT} is derived from the tectogrammatic layer of the
Prague Czech-English Dependency Treebank \cite{hajic_building_1998}.
See Figure~\ref{fig:formalisms} for an example sentence.
\begin{figure}
	\centering
		\includegraphics[width=.5\textwidth]{fig/example_dm} \\
		\includegraphics[width=.5\textwidth]{fig/example_pas} \\
		\includegraphics[width=.5\textwidth]{fig/example_pcedt}
	\caption{Example annotations for \emph{DM} (top), \emph{PAS}
          (middle), and \emph{PCEDT} (bottom).}
	\label{fig:formalisms}
\end{figure}

The three formalisms come from very different linguistic theories, but all
three are represented as labeled directed graphs, with words as vertices, and
all three have ``top'' annotations, corresponding roughly to the
semantic focus of the sentence.  A ``top'' need not be a root of the
graph.
This allows us to use the same machinery (\S\ref{s:models}) to simultaneously
train and test statistical models for the three formalisms.
\sam{Stats about \% multiple roots, \% multiple tops, \% tree, \% acyclic}



\section{Models} \label{s:models}

We treat the problem as a three-stage pipeline.
The first stage prunes words by predicting whether they have any incoming or
outgoing edges at all (\S\ref{s:singleton_model}); if a word does not, then it need not be 
considered in later stages.
The second stage jointly predicts whether an edge is
present, and if so, its label (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
Formalisms sometimes annotate more than one ``top'' per sentence, but we
found that we achieve the best performance on all formalisms by predicting only
the one best-scoring ``top'' under the model.
\bocomment{Singleton pruning does not matter for LogitEdge.  It doesn't affect accuracy, I'm pretty sure.  But it is essential for the graph model.}



\subsection{Singleton Classification} \label{s:singleton_model}

We call a node in a graph without any parents or children a \emph{singleton}.
For singleton prediction, we train a token-level logistic regression
classifier.
For each token $t$, the features include the token $t$, its lemma, and its
part-of-speech tag.
If the classifier predicts a probability of 99\% or higher (a
threshold chosen by grid search on the development set), then we prune the
token.
% Our singleton model achieves over 99\% accuracy for the \emph{PAS} formalism,
% for example.
We found that pruning words predicted to be singletons improved accuracy in later stages, and made the system
faster overall.
% \jdcomment{I need to find the accuracy for the other two formalisms. Will
% include this in our Results section.}




\subsection{Edge Prediction} \label{s:edge_model}

In the second stage of the pipeline, we predict the set of labeled directed
edges in the graph.
We use the same set of edge-factored features (\S\ref{s:features}) in two
alternative models: an edge-independent multiclass logistic
regression model (\logitedge, \S\ref{s:logitedge}); and a structured SVM 
\cite{taskar_max_2003,tsochantaridis_support_2004} that enforces 
\emph{determinism}\jmf{I'll find a citation} constraints that allow a word to
have at most one outgoing edge with the
same label (\svmedge, \S\ref{s:graphparser}).
For each formalism, we trained both models with varying features enabled and
hyperparameter settings and submitted the configuration that produced the best
labeled $F_1$ on the development set.
For \emph{DM} and \emph{PCEDT}, this was \logitedge;
for \emph{PAS}, this was \svmedge.
We report results only for the submitted configurations, with different features
enabled.
Due to time constraints, full hyperparameter sweeps and comparable feature sweeps were not possible.


\subsubsection{\logitedge\ Parser}
\label{s:logitedge}


\codenote{LRParser.java}

The \logitedge\ model considers only token index pairs $(i, j)$ where %that are
% within 10 tokens of each other (i.e.~
$|i-j| \leq 10$, $i \ne j$, and both $t_i$ and
$t_j$ have been predicted to be non-singletons by the first stage.
% In other words, edges with more than 9 tokens between its two endpoints are not
% considered.
Although this prunes some gold edges, among the formalisms,
95\%-97\% of all gold edges are between tokens of distance 10 or less.
We found that pruning long edges increased accuracy overall, in addition to
speeding up training and prediction.
Both directions $i \rightarrow j$ and $j \rightarrow
i$ are considered between every pair.

%\noindent
The model treats every ordered pair of token indices $(i, j)$ as a
multiclass logistic regression with $K+1$ possible outputs:
the formalism's original $K$ edge labels plus the additional label \noedge,
which indicates that no edge exists from $i$ to $j$.
Let $L$ be the set of possible edge labels.

% In our implementation, we separate features into two classes:
% those which can be extracted for any one token, $\bm{f}^{(\text{node})}(i)$;
% and those which look at both the parent and the child,
% $\bm{f}^{(\text{edge})}(i, j)$.
% These features are described in \S\ref{s:features}.
% Then the final feature vector $\bm{f}(i, j)$ is $\bm{f}^{(\text{edge})}(i,
% j)$, along with the $\bm{f}^{(\text{node})}$ features for both the parent and
% child token.
% The final feature vector is made by conjoining the output label with
% each:
% \[
% 	\bm{f}(i, j, \ell) := \text{conjoin}(\ell, \bm{f}^{\text{percept}}(i, j))
% \]
% where
% 
% \begin{aligned}
% \bm{f}^{\text{percept}}(i, j) := & \bm{f}^{(\text{edge})}(i, j) \\
% 	& \cup \text{ conjoin}(\text{``par''}, \bm{f}^{(\text{node})}(i)) \\
% 	& \cup \text{ conjoin}(\text{``child''}, \bm{f}^{(\text{node})}(j))
% \end{aligned}
% \[
% \text{conjoin}(\alpha, M) := \{ ((\alpha, k), v) \mid (k, v) \in M \}
% \]
%Feature extractors are defined to extract
%features both for individual tokens ($f^{(\text{node})}(i)$), and also for
% pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output
%labels.
% \noindent
For candidate parent index $i$ and child index $j$, we extract a feature vector
$\bm{f}(i, j)$, as described in \S\ref{s:features}.
The multiclass logistic regression model defines a distribution over $L$,
parametrized by the weight vectors $\Phi = \{\bm\phi_\ell\}_{\ell \in L}$:
\[
  P_\Phi(\ell)  = \frac{
  	\exp\{\bm\phi_\ell \cdot \bm{f}(i, j)\}
  } {
  	\sum_{\ell^\prime \in L} {
  		\exp\{\bm\phi_{\ell^\prime} \cdot \bm{f}(i, j)\}
  	}
  }
\]

\noindent
%In this notation, an $f$ function outputs a real-valued vector, defined as the
% concatenation of features from all the feature classes (\S\ref{s:features}).
$\Phi$ is learned by minimizing total negative log-likelihood of the above
(with weighting; see below), plus $\ell_2$ regularization.
Adagrad \cite{duchi_adaptive_2011} is used for optimization.
This seemed to optimize faster than L-BFGS \cite{byrd_limited_1995}, at least for earlier
iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

There is a class imbalance in the output labels; in all three formalisms, there
are many more $\noedge$ examples than true edge examples.
\sam{If we get these numbers, it would be nicer to quantify, and say something
like ``In the FFF formalism, the training set (\S\ref{s:evaluation}) contains NNN
candidate edges (pairs of tokens with length between 1 and 10 inclusive), and NNN actual (non-null) edges.''}
We improved $F_1$ performance by
downweighting $\noedge$ examples through a weighted log-likelihood objective,
$\sum_{i,j} \sum_\ell w_\ell \log P(y_{i,j}=\ell; \bm\phi)$, 
with $w_{\noedge}=0.3$ (selected on development set)
and $w_{\ell} = 1$ otherwise.

% 0.4

% Besides the edge logistic regression system, there were both pre- and post-processing steps.

% \textbf{Preprocessing:}
% \bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

% \textbf{Decoding and postprocessing:}
\textbf{Decoding:} \codenote{MyGraph::decodeEdgeProbsToGraph()}
To predict a graph structure at test-time for a new sentence,
the most likely edge label is predicted for every candidate $(i, j)$ pair of
tokens that has not been pruned by an earlier stage.
We enforce only one graph constraint, which is that there cannot be
an edge in both directions between any pair of words.
If an edge is predicted for both directions for a single $(i, j)$
pair, only the edge with the higher score is chosen.
There are no such bidirectional edges in the training data.
This enforcement actually did not improve accuracy on \emph{DM} or \emph{PCEDT};
it did improve \emph{PAS} by $\sim 0.2\%$ absolute $F_1$-score, but we did not submit \logitedge\ for \emph{PAS}.
\codenote{\url{https://github.com/Noahs-ARK/semeval-2014/pull/21}}


\subsubsection{\svmedge~Parser}
\label{s:graphparser}


To predict edges for the \emph{PAS} formalism, we use a structured SVM
with determinism constraints.
This ensures that each word token has at most one outgoing edge for each label
type.  We found these contraints to improve the $F_1$-score for this formalism only.
\bocomment{need to answer here: why only PAS?}

Consider the fully dense graph of all edges between all words predicted
as not singletons by the singleton classifier \S\ref{s:singleton_model} (in all
directions with all possible labels).
If $\Psi = \{\bm\psi_\ell\}_{\ell \in L}$ denotes the model
weights, and $\bm{f}$ denotes the same features as the
\logitedge~model, then an edge from $i$ to $j$ with label $\ell$ in the
dense graph has a weight $c(i,j,\ell)$ assigned to it using the linear
scoring function: \nas{no-edge is not included here, I am guessing.
  say so explicitly since this is a subtle difference with the logit model}
\[
c(i,j,\ell) = \bm\psi_\ell \cdot \bm{f}(i,j)
\]
%  Let $edge(i,j,\ell)$ be a
% binary function where $edge(i,j,\ell) = 1 $ denotes inclusion of an edge
% from $i$ to $j$ with a label $\ell$.  Then $edge(i,j,\ell)$ is computed
% as follows:
% \[
% edge(i,j,\ell) =
% \begin{cases}
% 1 & \text{if } j = \argmax_{k} c(i,k,\ell) \\
% & \text{and } c(i,j,\ell) > 0 \\
% 0 & \text{otherwise}
% \end{cases}
% \]
% In other words, f
For each node and each label, the decoder searches for
the highest scoring outgoing edge and adds it if its weight is
positive.
This procedure is guaranteed to find the highest scoring subgraph (largest sum
of edge weights) of the dense graph subject to the determinism constraints.
Its runtime is $O(n^2)$.



The model weights are trained using the structured SVM loss.  If $x$
is a sentence and $y$ is a graph over that sentence, let the features 
be denoted $\bm{f}(x,y) = \sum_{(i,j,\ell) \in edges(y)}
\bm{f}(i,j,\ell)$.  The SVM loss for each training example $(x_i, y_i)$ is:
\begin{multline*}
-\bm\psi^\top \bm{f}(x_i,y_i) + \max_{y} \bm\psi^\top \bm{f}(x_i,y) +
\mathit{cost}(y,y_i)
\end{multline*}
where $\mathit{cost}(y,y_i)$
is the cost function
\begin{multline*}
\mathit{cost}(y,y_i) = \alpha |edges(y)\setminus edges(y_i)| + \\
\beta |edges(y_i)\setminus edges(y)|.
\end{multline*}
$\alpha$ and $\beta$ trade off between precision and recall for the
edges.  The loss is minimized with AdaGrad
using early-stopping on a development set. % Tops are predicted using the
%top prediction model \S\ref{s:top_model}.


\subsubsection{Edge Features}
\label{s:edgefeatures}

\label{s:features}

\sam{would be nice to cite where we stole these features from, where applicable}
Table~\ref{table:edgefeatures} describes the features we used for predicting
edges.
These features were computed over an edge $e$ with parent token $s$ at
index $i$ and child token $t$ at index $j$. 
Unless otherwise stated, each feature template listed has an indicator
feature that fires for each value it can take on.  For the submitted results,
\logitedge~uses all features except Dependency Path v2, POS Path, and Distance
Thresholds, and \svmedge~uses all features except Dependency
Path v1.  This was due to \svmedge~being faster to
train than \logitedge~when including POS Path features, and due to time constraints for the submission we were unable to retrain
\logitedge~with these features.

\renewcommand{\floatpagefraction}{0.8}
\begin{table}
\begin{framed}
\begin{small}
%\textbf{Bias:} Always fires.

\textbf{Tokens:} The tokens $s$ and $t$ themselves.

\textbf{Lemmas:} Lemmas of $s$ and $t$.

\textbf{POS tags:} Part of speech tags of $s$ and $t$.

\textbf{Linear Order:} Fires if $i < j$.

\textbf{Linear Distance:} $i - j$.

\textbf{Dependency Path v1 (\logitedge~only):} The concatenation of all POS
tags and arc labels on the labeled path in the syntactic dependency tree from $s$ to
$t$.  Conjoined with $s$, with $t$, and without either.

\textbf{Dependency Path v2 (\svmedge~only):}
Same as Dependency Path v1, but with the lemma of $s$ or $t$ instead of the
word, and substituting the token for any ``IN'' POS tag.

\textbf{Up/Down Dependency Path:} The sequence of upward and
	downward moves needed to get from $s$ to $t$ in the syntactic dependency
	tree.

\textbf{Up/Down/Left/Right Dependency Path:} The unlabeled path through
	the syntactic dependency tree from $s$ to $t$, annotated with whether the each
	step through the tree was up or down, and whether it was to the right or left
	in the sentence.

\textbf{Is Parent:} Fires if  $s$ is the parent of $t$ in the syntactic
	dependency parse.

\textbf{Dependency Path Length:} Distance between $s$ and $t$ in the
	syntactic dependency parse.

\textbf{POS Context:} Concatenated POS tags of tokens at $i-1$, $i$,
	$i+1$, $j-1$, $j$, and $j+1$. Concatenated POS tags of tokens at $i-1, i, j-1$,
	and $j$. Concatenated POS tags of tokens at $i, i+1, j$, and $j+1$.

\textbf{Subcategorization Sequence:} 
	The sequence of dependency arc labels out of $s$, ordered by the index of the
	child.
	Distinguish left children from right children.
	If $t$ is a direct child of $s$, distinguish its arc label with a ``+''.
	Conjoin this sequence with the POS tag of $s$.

\textbf{Subcategorization Sequence with POS:} As above, but add the
	POS tag of each child to its arc label.

\textbf{POS Path (\svmedge~only):}
Concatenated POS tags between and including $i$ and $j$.
Conjoined with head lemma, with dependent lemma, and without either.


\textbf{Distance Thresholds (\svmedge~only):}
Fires for every integer between $1$ and $\lfloor \log(|i-j|+1)/\log(1.39)
\rfloor$ inclusive,
e.g.~$\{1,2,3,5,7,10,\ldots\}$.

\end{small}
\end{framed}
\caption{Features used in edge prediction}
\label{table:edgefeatures}
\end{table}





\subsubsection{Feature Hashing}

%\bocomment{ALTERNATE VERSION TO SAVE SPACE:
  The biggest memory usage was in the hash table implementing the map
  of feature names to integer indexes, used during feature extraction.
  In order to facilitate more convenient experimentation, we
  implemented multitask feature hashing
  \cite{weinberger_feature_2009}, which randomly assigns features'
  numeric indexes with a hash function, under the theory that errors
  due to collisions tend to cancel each other out.  We found no
  drop in accuracy when using the technique.

% }
% \sam{I think we can get away with this short version}
% %% 7-10 million features (percepts) before label conjunction:
% % cab:~dbamman/semeval/mar3 % wc -l *.model
% %    7518732 mar3_recsplit.dm.model
% %    7038841 mar3_recsplit.pas.model
% %    9939549 mar3_recsplit.pcedt.model
% %   24497122 total

% \codenote{
% \url{https://github.com/Noahs-ARK/semeval-2014/pull/20}
% Code:
%   See use of flag LRParser.useHashing, esp LRParser::perceptNum() 
%   and Model::coefIdx(): hashed values before and after label conjunction.
% }

% \noindent
% For \logitedge, the biggest memory usage was from
% the hash table that implemented
% a map of feature names to integer indexes, used during feature extraction.
% Tens of millions of features (before conjunction) resulted in several gigabytes of memory use.  To facilitate more convenient experimentation,
% we eliminated this by using
% multitask feature hashing
% \cite{weinberger_feature_2009},
% which randomly assigns features' numeric indexes with a hash function.\footnote{We also tried replacing the hash table with a Patricia Tree, trying several open-source implementations; but they had worse memory usage than a GNU Trove hash table. 
%   \codenote{\url{https://github.com/brendano/myutil/blob/0a5697a7f066c265a75ce1b7bd527feafa683c8a/src/vocabalts/vocabalts_results.txt}}}
% Let $F$ be the space of all feature names (before conjunction with the output label).
% In the usual approach, a hash table is maintained containing a
% one-one onto map from features to integer indexes $g: F \rightarrow \{1,\ldots,|F|\}$,
% and each output label has an array of model parameters $\bm\phi^\ell \in \mathbb{R}^{|F|}$,
% which $g(f)$ indexes into.
% In the hashing approach, a hash function $h_{\ell}$
% maps feature name to one of $B$ bucket values,
% $h_{\ell} : F \rightarrow \{1, \ldots, B\}$.
% (A different hash function is used for each output label.)
% The model parameters are stored in an array $h_\ell(\bm\phi^\ell) \in \mathbb{R}^{B}$,
% for which an entry is defined as
% \[
%   h_\ell(\bm\phi^\ell)_{b} = 
%   \sum_{\substack{\text{feat} \in F 
%     \\\text{ s.t. } h_\ell(\text{feat})=b}}
%   {\phi^\ell_{\text{feat}}}
% \]
% This is easy to implement:
% array lookups and gradient steps are simply implemented with $h$ instead of $g$, and feature vectors are also stored in a hashed space.
% \cite{weinberger_feature_2009} show that, even with some collisions,
% dot products are approximately preserved with high probability;
% and indeed, we observed no drop in accuracy compared to using an explicit feature map.
% We used a value of $B$ as large as was convenient for memory usage (due only to $h(\phi)$), $B=10^{8}$.
% This is actually higher than $|F|\approx 10^7$, though plenty of collisions still happen: about 9.5\% of features are hashed to a value shared by at least one other feature.

% %% simulation:
% % num buckets count 0: 90484843
% % num buckets count 1: 9046266
% % num collisions: num features that are in a non-singleton bucket: 953734
% %%  average over simulations for last value: 9.518e+05 


\subsection{Top Prediction} \label{s:top_model}

\bocomment{may15 afternoon: The below describes how top pred works in LogitEdge.  Jeff, please confirm this is how you use the topness classifier, or modify appropriately.}\jmf{yes, this is correct for the graph parser}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all predicted predicates (i.e., nodes where there is at least one outbound edge)
are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top.''
\codenote{LogitEdge: MyGraph::decideTops()}
This is suboptimal, since some graphs have multiple tops (in \emph{PCEDT} this is
more common);
but selection rules based on probability thresholds gave worse $F_1$
performance on the development set. \codenote{\url{https://github.com/Noahs-ARK/semeval-2014/issues/37}}

For a given token $t$ at index $i$, the top classifier's features
included $t$'s POS tag, $i$, $i$, those two conjoined, and the depth
of $t$ in the syntactic dependency tree.


\section{Negative Results}
\label{s:badfeatures}

We followed a forward-selection process during feature engineering.
For each potential feature, we tested the current feature set versus the current
feature set plus the new potential feature.
If the new feature did not improve performance, we did not add it.
We list in table \ref{table:negedgefeatures} some of the features which we tested, but did not find to improve
performance.

In order to save time, we ran these feature selection experiments
on a subsample of the training data, for a reduced number of iterations.
These results thus have a 
 strong caveat that the experiments were
not exhaustive.  It could be the case that that any of these features would help under more careful testing.


\begin{table}
\begin{framed}
\begin{small}
%\textbf{Bias:} Always fires.

\textbf{Word vectors:} Features derived from 64-dimensional vectors from  \cite{wordVectors}, including their
concatenation, difference, inner product, and element-wise multiplication. Random forest\footnote{Using \cite{RandomForest2002}'s \emph{randomForest} R implementation.}
from the word vectors, using its predicted hard labels as features;

\textbf{Brown clusters} Features derived from Brown clusters \cite{Brown:1992:CNG:176313.176316}
trained on a large corpus of web data. Parent, child and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10 and 12. Conjunctions of those features with the POS tags of the parent and
child tokens.


\textbf{Active/passive:} Active/passive voice feature\newcite{johansson_dependency-based_2008} conjoined with both the Linear Distance features and the Subcategorization Sequence features (we hypothesize voice information may already have been incorporated into our featureset by the use of the supplied Stanford dependency-style parses, which include passivization information in arc labels such as \emph{nsubjpass},\emph{auxpass}).
features.

\textbf{Complex distance features:} Log distance, binned
distance (i.e., less than five tokens apart, between five and ten tokens apart, more than
ten tokens apart), and thresholded distance (i.e., zero to three tokens apart,
zero to five tokens apart, etc.).

\textbf{Connectivity constraint:} Enforcing that the graph was connected (ignoring singletons), similarly
to \newcite{flanigan-etal:ACL2014}. Almost all semantic dependency graphs are connected (ignoring singletons) in the
training data, but we found that enforcing this constraint causes more edges to be predicted, significantly hurting
performance.

\textbf{Tree constraint:} Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance.

\end{small}
\end{framed}
\caption{Negative features and graph constraints.}
\label{table:negedgefeatures}
\end{table}


% For each method outlined below, each dimension of the output vector was used as
% a feature, whose value was the value of the vector along that dimension.
% %As an example, if we concatenated the two vectors into a third vector $V_3$ (as
% %we show in $f_1$), each element of $V_3$ was used as a feature in our model.

% \begin{itemize}
% \item Concatenation:
% $ \left( \begin{smallmatrix} \bm{v_1}\\ \bm{v_2}
% \end{smallmatrix} \right)$
% \item Difference: $\bm{v_1} - \bm{v_2}$.
% \item Dot product: $\bm{v_1} \cdot \bm{v_2}$
% \item Element-wise multiplication: $\bm{v_1} \bigodot \bm{v_2}$
% \end{itemize}
% Unfortunately we saw no improvement from these features.



\section{Experiment Setup}
\label{s:evaluation}

We participated in the Open Track, and used the syntactic dependency parses supplied by the organizers.  Feature engineering was performed on a development set (\S 20), training on \S 00--19.
We evaluate labeled precision (LP), labeled recall (LR), labeled $F_1$ (LF), and
labeled whole-sentence match (LM) on the held out test data using the
evaluation script provided by the organizers. 
LF was averaged over the formalisms to determine the winning system.
Table~\ref{table:perf} shows our scores.

% CMU:
% DM:
% LP: 0.844641
% LR: 0.834849
% LF: 0.839716
% LM: 0.087537
% 
% 
% PAS:
% LP: 0.907832
% LR: 0.885141
% LF: 0.896343
% LM: 0.260386
% 
% 
% PCEDT:
% LP: 0.768139
% LR: 0.707173
% LF: 0.736396
% LM: 0.071217

% Priberam:
% DM:
% LP: 0.902322
% LR: 0.881050
% LF: 0.891559
% LM: 0.268546
% 
% PAS:
% LP: 0.925576
% LR: 0.909676
% LF: 0.917557
% LM: 0.378338
% 
% PCEDT:
% LP: 0.801397
% LR: 0.757900
% LF: 0.779042
% LM: 0.106825


\begin{table}
\begin{center}
\begin{tabular}%{\textwidth}%{.6\textwidth}
{@{\extracolsep{\fill}}c|cccc}%|c|c|c|c|c|c|}
% \hline
& \textbf{LP} & \textbf{LR} & \textbf{LF} & \textbf{LM} \\
\hline
\hline
\textbf{DM}
& 0.8446 & 0.8348 & 0.8397 & 0.0875 \\
\hline
\textbf{PAS}
& 0.9078 & 0.8851 & 0.8963 & 0.2604 \\
\hline
\textbf{PCEDT}
& 0.7681 & 0.7072 & 0.7364 & 0.0712 \\
\hline
\hline
\textbf{Average}
& 0.8402 & 0.8090 & 0.8241 & 0.1397 \\
\end{tabular}
\caption{Labeled precision (LP), recall (LR), $F_1$ (LF), and
whole-sentence match (LM) on the held out test data.
}
\label{table:perf}
\end{center}
\end{table}


\section{Conclusion and Future Work}
We found that feature-rich discriminative models perform well at the task of
mapping from sentences to semantic dependency parses. 
While our final approach is fairly standard for work in parsing, we
note here additional features and constraints which did not
appear to help (contrary to expectation).
There are a number of clear extensions to this work that could improve
performance.
While using an edge factored model allows for efficient inference, there is
much to be gained from using \textbf{higher-order features} \cite{mcdonald_online_2006,martins_turning_2013}
The amount of information
shared between the three formalisms suggests that a \textbf{multi-task learning} framework
could lead to gains.
And finally, there is additional structure in the formalisms which could be
exploited (such as the deterministic processes by which an original PCEDT tree annotation was converted into a graph); formulating more subtle \textbf{graph constraints} that are able to capture this a priori
knowledge of the graph structure could lead to improved performance.  We leave such explorations to future work.



\small
\section*{Acknowledgements}
We are grateful to Manaal Faruqui for his help in the word vector experiments.
The research reported in this article was sponsored by the U.S.~Army Research
Laboratory and the U.~S.~Army Research Office under contract/grant number
W911NF-10-1-0533, DARPA grant FA8750-12-2-0342 funded under the DEFT
program, U.S.~NSF grants IIS-1251131 and IIS-1054319, and Google's
support of the Reading is Believing project at CMU.


\bibliographystyle{acl}
\bibliography{semeval8,morebib}




\end{document}
