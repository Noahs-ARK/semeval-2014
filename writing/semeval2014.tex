%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{enumitem}
\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
% \usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\sam}[1]{\textcolor{blue}{[#1 -SMT]}}
\newcommand{\jdcomment}[1]{\textcolor{NavyBlue}{[#1 -JDD]}}

% turn off for submission, but on for a more tech report-y version.
\newcommand{\codenote}[1]{\textcolor{PineGreen}{[Code: \emph{#1}]}}
\newcommand{\logitedge}{\textsc{LogitEdge}}
\newcommand{\noedge}{\textsc{NoEdge}}


\title{Broad-Coverage Semantic Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present a graph-based approach for parsing to the three semantic dependency
formalisms in SemEval 2014 Shared Task 8: Broad-Coverage Semantic
Dependency Parsing.
  
\end{abstract}



\section{Introduction}

We desire shallow semantic analyzers which can analyze broad-domain text quickly
and accurately.
\begin{itemize}
\item applications of semantic parsing
\end{itemize}

Syntactic parsing has greatly benefited from the existence of a large standard
corpus of annotations \cite{marcus_building_1993}.
Dependency parsing is a popular form of syntactic processing, as it
is generally faster than constituency parsing, while still capturing much of
what is needed for downstream applications.
% allows for a simpler representation
But while syntactic parsers capture relationships between words in a sentence,
the relationships they capture are not semantic.
For instance, in ``The bread baked in the oven.'' and ``I baked the bread in the
ven.'' ``the bread'' bears the same semantic relationship to ``baked'' but is
realized in a different syntactic position.



\section{Formalisms}

\begin{itemize}
	\item Quick overview of three formalisms, emphasizing differences between them, consequences for parsing
	\item DM:
	\begin{itemize}
	  \item ERG \cite{flickinger_deepbank_2012}
	\end{itemize}
	\item PAS:
	\begin{itemize}
	  \item Enju\ldots \cite{miyao_corpus_oriented_2004}
	\end{itemize}
	\item PCEDT:
	\begin{itemize}
	  \item Tectogrammatic layer \cite{hajic_building_1998}
	\end{itemize}
\end{itemize}


\section{Models}

We treat the problem as a three-stage pipeline.
The first stage prunes words by predicting whether a word is a \emph{singleton}
or not--whether it has any incoming or outgoing edges at all (\S\ref{s:singleton_model}).
If a word is predicted to be a singleton, then it no longer needs to be
considered in later stages.
The second stage jointly predicts whether an edge is
present, and if so, what its label is (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
``Top'' annotations roughly correspond to the focus of a
sentence.


\subsection{Singleton Classification} \label{s:singleton_model}

For singleton prediction, we train a simple token-level logistic regression
classifier.
A singleton is a node in a graph without any parents or children.
We train one model for each of the three formalisms, as each of the three
formalisms follow different conventions determining whether a token would be
included in the final graph for a sentence.
In the PAS formalism, the rules to determine whether a token was a
singleton were relatively simple and deterministic, so our model achieved over 99\%
accuracy.
\jdcomment{I need to find the accuracy for the other two formalisms. Will
include this in our Results section.}

For each token $t$, the model considers the following features:
\begin{itemize}
\item the token $t$ itself
\item the lemma of $t$
\item the part of speech tag of $t$
\end{itemize}



\subsection{Edge Prediction} \label{s:edge_model}

In the second stage of the pipeline, we predict the set of labeled directed
edges in the graph.
We use the same set of (first-order) features
in two different models:
for the DM and PCEDT formalisms, we use an edge-independent multiclass logistic
regression model (\logitedge, \S\ref{s:logitedge}).
For the PAS formalism, we use a structured SVM 
\cite{taskar_max_2003,tsochantaridis_support_2004} that enforces a
\emph{determinism} constraint -- a word may not have two outgoing edges with the
same label (\S\ref{s:graphparser}).

Both models consider only tokens pairs $(i, j)$ where %that are within 10 tokens
%of each other (i.e.~
$|i-j| \leq 10$, $i \ne j$, and both $i$ and
$j$ have been predicted to be non-singletons by the first stage.
Although this prunes some gold edges, among the formalisms, 95\%-97\% of all
gold edges are between tokens of distance 10 or less.
Both directions $i \rightarrow j$ and $j \rightarrow i$ are considered between
every pair.


\subsubsection{\logitedge\ parser} \label{s:logitedge}


\codenote{LRParser.java}

\noindent
The \logitedge\ model treats every ordered pair of tokens $(i, j)$ as a
multiclass logistic regression with $K+1$ possible outputs:
either one of the formalism's $K$ edge labels, or
the additional label \noedge, which indicates that no edge exists from $i$ to
$j$.

Feature extractors are defined to extract features both for individual tokens
($f^{(\text{node})}(i)$), and also for pairs of tokens
($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output
labels.
For candidate head $i$ and child $j$, the model defines a distribution over
$y_{ij} \in \{\text{edge labels}\} \cup \{\noedge\}$, using $3$ weight matrixes
$\beta$,

\begin{align*} 
  P(y_{ij}=k; \beta) & \propto 
  \exp \big( \beta^{\text{(bias)}}_k \ \ + \\
  &
  \beta^{\text{(node as head)}}_k \cdot f^{\text{(node)}}(i)
  \ \ + \\ 
  &
  \beta^{\text{(node as child)}}_k \cdot f^{\text{(node)}}(j)
  \ \ + \\
  &
  \beta^{\text{(edge)}}_k \cdot f^{\text{(edge)}}(i,j)
    \big)
\end{align*}

\noindent
In this notation, an $f$ function outputs a real-valued vector, defined as the concatenation of features from all the feature classes (\S\ref{s:features}).
Training minimizes total negative log-likelihood of the above (with weighting; see below),
plus $\ell_2$ regularization.  Adagrad \cite{duchi_adaptive_2011} is used for optimization.
It seemed to optimize faster than L-BFGS\sam{cite}, at least for earlier
iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

In the FFF formalism, the training set (\S\ref{s:datasplits})
contains NNN candidate edges (pairs of tokens with length between 1 and 10 inclusive),
and NNN actual (non-null) edges.  $F$-score performance was improved by
downweighting $\noedge$ examples through a weighted log-likeihood objective,
$\sum_{i,j} \sum_k w_k \log P(y_{ij}=k;\beta)$, with $w_{\noedge}=0.3$ and $1$ otherwise.  
(The weight was chosen by gridsearch on a development set; 
PAS used a weight of $0.4$ while the others were $0.3$, though the differences at this granularity were small.)

% Besides the edge logistic regression system, there were both pre- and post-processing steps.

% \textbf{Preprocessing:}
% \bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

% \textbf{Decoding and postprocessing:}
\textbf{Decoding:}
\codenote{decodeEdgeProbsToGraph(), MyGraph.java}

To predict a graph structure at test-time for a new sentence,
the most likely edge label is predicted for every candidate $(i, j)$ pair of
tokens that has not been pruned by an earlier stage.
%There were several post-processing steps.
We enforce only one graph constraint, which is that there cannot be
an edge in both directions between any pair of words.
If an edge is predicted for both directions for a single $(i, j)$
pair, only the edge with the higher score is chosen.
% If the gold standard has an edge in the direction $i \rightarrow j$, the
% direction $j \rightarrow i$ is considered a \noedge.
There are no such bidirectional edges in the training data, and enforcing this
restriction improved accuracy on the development set.%, though it is only a
% primitive constraint on the graph decoding.



\subsubsection{Graph-based Structured SVM Parser} \label{s:graphparser}

To predict edges for the PAS formalism, we use a structured SVM model
\sam{cite}.
The model uses the same set of features as \logitedge, but optimizes a
max-margin objective during training, and enforces an additional constraint: no
node may have more than one outgoing edge with the same label.

\sam{TK from jflan}


\subsubsection{Edge Features}

\label{s:features}
These features were computed over an edge $E$ with source token $S$ and target token $T$.  For each of those listed here, we have an indicator feature for each value it takes on. 

\begin{itemize}
\item Basic
\begin{itemize}
\item Lemmas of $S$ and $T$.
\item Part of speech tags of $S$ and $T$.
\item Token strings of $S$ and $T$.
\item 1 if Index($S$) $\le$ Index($T$), 0 otherwise.
\end{itemize}
\item LinearOrder
\begin{itemize}
\item Index($S$) - Index($T$).
\end{itemize}
\item CoarseDependency
\begin{itemize}
\item 1 if $S$ is the parent of $T$ in syntactic dependency parse, 0 otherwise.
\item Distance between $S$ and $T$ in syntactic dependency parse.
\end{itemize}
\item LinearContext
\begin{itemize}
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($S$)+1, Index($T$)-1, Index($T$), Index($T$)+1.
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($T$)-1, Index($T$).
\item Concatenated PoS tags of tokens at Index($S$), Index($S$)+1, Index($T$), Index($T$)+1.
\end{itemize}
\item DependencyPath
\begin{itemize}
\item The token string concatenated to the labeled path through the syntactic dependency tree from $S$ to $T$.
\end{itemize}
\item SubcatSequence
\begin{itemize}
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the label of the ark to the child. If $c$ is $T$, append a ``+''.
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the PoS tag of $c$ and the label of the ark to the child. If $c$ is $T$, append a ``+''.
\end{itemize}
\item UnlabeledDep
\begin{itemize}
\item The unlabeled path through the syntactic dependency tree from $S$ to $S$. 
\item The unlabeled path through the syntactic dependency tree from $S$ to $T$, annotated with whether the each step through the tree was to the right or left in the sentence.
\end{itemize}
\item Brown clusters
\begin{itemize}
\item \jdcomment{The word on the street is that these weren't used. Check on that, then consider moving to ``didn't work'' section.}
\end{itemize}
\end{itemize}

\subsubsection{Feature Hashing}

During our experiments, the number of instantiated features ranged into the tens
of millions.
As a memory optimization, we implemented feature hashing
\cite{weinberger_feature_2009}.
Let $F$ be the space of all feature names.
Instead of storing our model parameters as a hashmap %from
% \texttt{feature\_name} to \texttt{feature\_value},
or array $\phi$ of size $|F|$, we instead store them in an array $\psi$ of size
$k$, with $k < |F|$.
We choose a hash function $h : F \rightarrow \{0, \ldots, k-1\}$, and
let the $i^{th}$ component of $\psi$ be 
\[
\psi_i = \sum_{\text{feat}\in F \text{ s.t. }
h(\text{feat})=i}{\phi_{\text{feat}}}
\]
By never explicitly storing the full set of feature names, we can use
tens of millions of features while keeping a fixed memory overhead.
The downside is that whenever there is a collision, and two features hash to the
same value, our model can no longer differentiate between those two features.
In practice, we choose large enough $k$ such that performance is unaffected by
collisions.



\subsection{Top Prediction} \label{s:top_model}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all nodes in the predicted graph (i.e., tokens where there is
either an inbound or outbound edge) are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top''.
This is suboptimal, since some graphs have multiple tops (and in PCEDT this is
more common);
but selection rules based on probability thresholds gave worse F-score
performance on the devset. \bocomment{I wonder if I documented this on an issue/PR}


\subsubsection{Top Features}
For a given token $T$, the topness classifier used the following features:
\begin{itemize}
\item The PoS tag of $T$.
\item Index($T$).
\item Conjoined PoS tag of $T$ and Index($T$).
\item The depth of $T$ in the syntactic dependency parse. 
\end{itemize}






\section{Negative Results}

We followed a forward-selection process during feature engineering.
For each potential feature, we tested the current feature set versus the current
feature set plus the new potential feature.
If the new feature did not improve performance, we did not add it.
We list below some of the features which we tested, but did not find to improve
performance.

Note that in order to save time, we ran these feature selection experiments
on a subsample of the training data, for a reduced number of iterations.
So please regard the following with the strong caveat that our experiments were
not exhaustive, and we do not claim that these features could not be found to
help.

\subsection{Word vectors}
Word vectors have been shown to benefit a number of semantic tasks. We
incorporated features based on trained word vectors in a number of ways, though
in this task we did not find benefit from any of them.
The vectors we used \cite{wordVectors} were trained trained on the WMT-2011
dataset, using a variation on latent semantic analysis which incorporated
information from multiple languages.
The vectors themselves contained 64 dimensions.

For our edge factored model \logitedge, when considering a potential edge, we
had a source word $W_1$ with associated word vector $V_1$ and a target word
$W_2$ with associated word vector $V_2$.
For each method outlined below, the elements of the output vector was used as a
feature.
As an example, if we concatenated the two vectors into a third vector $V_3$ (as
we show in $f_1$), each element of $V_3$ was used as a feature in our model.

\begin{enumerate}
\item $f_1(V_1,V_2)$ = $ \left( \begin{smallmatrix} V_1\\ V_2 \end{smallmatrix} \right)$
\item $f_2(V_1,V_2)$ = $V_1 - V_2$.
\item $f_3(V_1,V_2)$ = $V_1 \cdot V_2$ (Dot product)
\item $f_4(V_1,V_2)$ = $V_1 \bigodot V_2$ (Element-wise multiplication)
\end{enumerate}
Unfortunately we saw no improvement from these features.

\subsection{Random Forests Over Word Vectors}
The word vectors added as features didn't reliably improve accuracy on any of
the formalisms.
One hypothesis as to why was that the vectors had a non-linear relationship
with the labels.
To address this, we trained a random forest in
R\footnote{http://cran.r-project.org/web/packages/randomForest/randomForest.pdf}
with the word vector features as described above, and used the hard labels
predicted by the random forests as features in the \logitedge\ parser.
Again, we saw no improvement.

\bocomment{caveat, these were not carefully done.}


\section{Evaluation}
\label{s:datasplits}


% DM:
% LP: 0.844641
% LR: 0.834849
% LF: 0.839716
% LM: 0.087537
% 
% 
% PAS:
% LP: 0.907832
% LR: 0.885141
% LF: 0.896343
% LM: 0.260386
% 
% 
% PCEDT:
% LP: 0.768139
% LR: 0.707173
% LF: 0.736396
% LM: 0.071217


\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
formalism &
\multicolumn{1}{c}{P} & \multicolumn{1}{c}{R} &
\multicolumn{1}{c}{$F_1$} &
\multicolumn{1}{c|}{Whole Sentence} \\
\hline
DM & 0.8446 & 0.8348 & 0.8397 & 0.0875 \\
\hline
PAS & 0.9078 & 0.8851 & 0.8963 & 0.2604 \\
\hline
PCEDT & 0.7681 & 0.7072 & 0.7364 & 0.0712 \\
\hline
\end{tabular}
\caption{End-to-end performance.}
\label{table:perf}
\end{center}
\end{table}



\section{Future work}

\begin{itemize}
  \item Higher-order features
  \item Multitask learning
\end{itemize}


\section{Conclusion}

\nocite{flanigan-etal:ACL2014}



\section*{Acknowledgements}

\bibliographystyle{acl}
\bibliography{semeval8}




\end{document}
