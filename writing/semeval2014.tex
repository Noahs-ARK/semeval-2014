%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}

% turn off for submission, but on for a more tech report-y version.
\newcommand{\codenote}[1]{\textcolor{PineGreen}{[Code: \emph{#1}]}}

\title{SemEval-2014 Task 8: Broad-Coverage Semantic Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  
\end{abstract}



\section{Introduction}

We present a graph-based approach to semantic dependency parsing for SemEval
2014 Shared Task 8.

\section{Formalisms}

\begin{itemize}
\item Quick overview of three formalisms, emphasizing differences between them, consequences for parsing
\end{itemize}


\section{Models}
\newcommand{\logitedge}{\textsc{LogitEdge}}

We treat the problem as a three-stage pipeline.
The first stage predicts whether a word is a \emph{singleton} or not --
whether it has any incoming or outgoing edges at all
(\S\ref{s:singleton_model}).
The second stage jointly predicts whether an edge is present, and if so, what
its label is (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
``Top'' annotations roughly correspond to the focus of a
sentence.


\subsection{Singleton Prediction} \label{s:singleton_model}

We use a logistic regression model, tuned for high precision.


\subsection{Edge Prediction} \label{s:edge_model}

We used the same set of (first-order) features
in two different models: an edge-independent multiclass %a faster-to-train
logistic regression model (\logitedge, \ref{s:logitedge}) for the DM and PCEDT
formalisms, and a discriminative structured graph prediction model
(\ref{s:graphparser}) for the PAS formalism.


\subsubsection{\logitedge\ parser} \label{s:logitedge}


\codenote{LRParser.java}

\noindent
This model treats every pair of tokens as a multiclass logistic regression
with $K+1$ possible outputs:
either to be an edge with one of the formalism's $K$ different labels, or a null
\textsc{NoEdge} decision.  

The model considers all token pairs $(i,j)$ within distance 10 of each other, 
i.e.~$1\leq|i-j|\leq 10$, \bocomment{and excluding pruning (ref later subsection)}. Among the formalisms, 95\%-97\% of all edges are distance 10 or less.
Both directions are considered.  If the gold standard has an edge in the direction $i \rightarrow j$, the direction $j\rightarrow i$ is considered a \textsc{NoEdge}.

Feature extractors are defined to extract features both for individual tokens ($f^{(\text{node})}(i)$), and also for pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output labels.
For candidate head $i$ and child $j$, the model defines a distribution over
$y_{ij} \in \{\text{edge labels}\} \cup \{\textsc{NoEdge}\}$, using $3$ weight matrixes $\beta$,

\begin{align*} 
  P(y_{ij}=k; \beta) & \propto 
  \exp \big( \beta^{\text{(bias)}}_k \ \ + \\
  &
  \beta^{\text{(node as head)}}_k \cdot f^{\text{(node)}}(i)
  \ \ + \\ 
  &
  \beta^{\text{(node as child)}}_k \cdot f^{\text{(node)}}(j)
  \ \ + \\
  &
  \beta^{\text{(edge)}}_k \cdot f^{\text{(edge)}}(i,j)
    \big)
\end{align*}

\noindent
In this notation, an $f$ function outputs a real-valued vector, defined as the concatenation of features from all the feature classes (\S\ref{s:features}).
Training minimizes total negative log-likelihood of the above (with weighting; see below),
plus $\ell_2$ regularization.  Adagrad \cite{duchi2011adaptive} is used for optimization.
It seemed to optimize faster than L-BFGS, at least for earlier iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

In the FFF formalism, the training set (\S\ref{s:datasplits})
contains NNN candidate edges (pairs of tokens with length between 1 and 10 inclusive),
and NNN actual (non-null) edges.  $F$-score performance was improved by downweighting $\textsc{NoEdge}$ examples through a weighted log-likeihood objective,
$\sum_{i,j} \sum_k w_k \log P(y_{ij}=k;\beta)$,
with $w_{\textsc{NoEdge}}=0.3$ and $1$ otherwise.  
(The weight was chosen by gridsearch on a development set; 
PAS used a weight of $0.4$ while the others were $0.3$, though the differences at this granularity were small.)

Besides the edge logistic regression system, there were both pre- and post-processing steps.

\textbf{Preprocessing:}
\bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

\textbf{Decoding and postprocessing:}
\codenote{decodeEdgeProbsToGraph(), MyGraph.java}
To predict a graph structure at test-time for a new sentence,
the most likely edge label was predicted for every candidate $(i,j)$ pair of tokens satisfying the distance restriction.  There were several post-processing steps.

First, if a non-\textsc{NoEdge} edge was predicted for both directions for a single $(i,j)$ pair, only the more likely one was used for the final graph.  This improved devset accuracy, though it is only a primitive constraint on the graph decoding.



\subsubsection{Graph-based parser} \label{s:graphparser}

\subsection{Top Prediction} \label{s:top_model}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all nodes in the predicted graph (i.e., tokens where there is
either an inbound or outbound edge) are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top''.
This is suboptimal, since some graphs have multiple tops (and in PCEDT this is
more common);
but selection rules based on probability thresholds gave worse F-score
performance on the devset. \bocomment{I wonder if I documented this on an issue/PR}

\section{Features}

\subsection{all the stuff in our model}

\label{s:features}

\begin{itemize}
\item BasicFeatures
\item LinearOrderFeatures
\item CoarseDependencyFeatures
\item LinearContextFeatures
\item DependencyPathv1
\item SubcatSequenceFE
\item UnlabeledDepFE
\item Brown clusters
\end{itemize}
\subsection{feature hashing}


\subsection{Features for subsystem classifiers}

 - Topness classifier

 - Singleton classifier

 - Predicate-ness classifier


\subsection{all the stuff we tried but didn't work}

\bocomment{caveat, these were not carefully done.}


\section{Evaluation}
\label{s:datasplits}

\section{Future work}

\section{Conclusion}

\nocite{flanigan-etal:ACL2014}



\section*{Acknowledgements}

\bibliographystyle{acl}
\bibliography{semeval8,morebib}




\end{document}
