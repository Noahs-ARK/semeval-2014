%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

\usepackage{enumitem}
\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[usenames,dvipsnames]{color}
\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\jdcomment}[1]{\textcolor{NavyBlue}{[#1 -JDD]}}

% turn off for submission, but on for a more tech report-y version.
\newcommand{\codenote}[1]{\textcolor{PineGreen}{[Code: \emph{#1}]}}

\title{SemEval-2014 Task 8: Broad-Coverage Semantic Dependency Parsing}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  {\tt email@domain} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
  abstract
\end{abstract}



\section{Introduction}



\section{Formalisms}

\begin{itemize}
\item Quick overview of three formalisms, emphasizing differences between them, consequences for parsing
\end{itemize}


\section{Models}
\newcommand{\logitedge}{\textsc{LogitEdge}}

We used the same set of (first-order) features
in two different models: a faster-to-train
logistic regression model (\logitedge, \ref{s:logitedge}),
and a structured graph prediction model (\ref{s:graphparser}).


\subsection{\logitedge\ parser} \label{s:logitedge}

\codenote{LRParser.java}

\noindent
This model treats every pair of tokens as a multiclass logistic regression
with $K+1$ possible outputs:
either to be an edge with one of the formalism's $K$ different labels, or a null
\textsc{NoEdge} decision.  

The model considers all token pairs $(i,j)$ within distance 10 of each other, 
i.e.~$1\leq|i-j|\leq 10$, \bocomment{and excluding pruning (ref later subsection)}. Among the formalisms, 95\%-97\% of all edges are distance 10 or less.
Both directions are considered.  If the gold standard has an edge in the direction $i \rightarrow j$, the direction $j\rightarrow i$ is considered a \textsc{NoEdge}.

Feature extractors are defined to extract features both for individual tokens ($f^{(\text{node})}(i)$), and also for pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output labels.
For candidate head $i$ and child $j$, the model defines a distribution over
$y_{ij} \in \{\text{edge labels}\} \cup \{\textsc{NoEdge}\}$, using $3$ weight matrixes $\beta$,

\begin{align*} 
  P(y_{ij}=k; \beta) & \propto 
  \exp \big( \beta^{\text{(bias)}}_k \ \ + \\
  &
  \beta^{\text{(node as head)}}_k \cdot f^{\text{(node)}}(i)
  \ \ + \\ 
  &
  \beta^{\text{(node as child)}}_k \cdot f^{\text{(node)}}(j)
  \ \ + \\
  &
  \beta^{\text{(edge)}}_k \cdot f^{\text{(edge)}}(i,j)
    \big)
\end{align*}

\noindent
In this notation, an $f$ function outputs a real-valued vector, defined as the concatenation of features from all the feature classes (\S\ref{s:features}).
Training minimizes total negative log-likelihood of the above (with weighting; see below),
plus $\ell_2$ regularization.  Adagrad \cite{duchi2011adaptive} is used for optimization.
It seemed to optimize faster than L-BFGS, at least for earlier iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

In the FFF formalism, the training set (\S\ref{s:datasplits})
contains NNN candidate edges (pairs of tokens with length between 1 and 10 inclusive),
and NNN actual (non-null) edges.  $F$-score performance was improved by downweighting $\textsc{NoEdge}$ examples through a weighted log-likeihood objective,
$\sum_{i,j} \sum_k w_k \log P(y_{ij}=k;\beta)$,
with $w_{\textsc{NoEdge}}=0.3$ and $1$ otherwise.  
(The weight was chosen by gridsearch on a development set; 
PAS used a weight of $0.4$ while the others were $0.3$, though the differences at this granularity were small.)

Besides the edge logistic regression system, there were both pre- and post-processing steps.

\textbf{Preprocessing:}
\bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

\textbf{Decoding and postprocessing:}
\codenote{decodeEdgeProbsToGraph(), MyGraph.java}
To predict a graph structure at test-time for a new sentence,
the most likely edge label was predicted for every candidate $(i,j)$ pair of tokens satisfying the distance restriction.  There were several post-processing steps.

First, if a non-\textsc{NoEdge} edge was predicted for both directions for a single $(i,j)$ pair, only the more likely one was used for the final graph.  This improved devset accuracy, though it is only a primitive constraint on the graph decoding.

Second, the edge-based logistic regression has no facility for the ``top'' or root-like attribute for a graph node.  We trained a separate token-level binary logistic regression to classify whether a token's node had the ``top'' attribute or not.  At decoding time, all nodes in the predicted graph (i.e., tokens where there is either an inbound or outbound edge) are possible candidates to be ``top''; the classifier probabilities are evaluated, and the highest-scoring node is chosen to be ``top''.
This is suboptimal, since some graphs have multiple tops (and in PCEDT this is more common); but selection rules based on probability thresholds gave worse F-score performance on the devset. \bocomment{I wonder if I documented this on an issue/PR}


\subsection{Graph-based parser} \label{s:graphparser}

\subsection{Singleton Classification}
For singleton prediction, we again use logistic regression. A singleton is a node in a graph without any parents or children. We trained one model for each of the three formalisms, as each of the three formalisms used different rules to determine whether a token would be included in the final graph for a sentence. In one formalism, PAS, the rules to determine whether a token was a singleton were relatively simple and deterministic, so our model achieved over 99\% accuracy. \jdcomment{I need to find the accuracy for the other two formalisms. Will include this in our Results section.}

\subsubsection{Features}
As this model only predicted whether a given token was a singleton, it used a subset of the features that the edge parsers used. Specifically, this model used the BasicFeatures feature set. 


\subsection{Topness Classification}
In each of the three formalisms, there is exactly one node that is at the top of the graph. 

\subsubsection{Topness Features}
For a given token $T$, the topness classifier used the following features:
\begin{itemize}
\item The PoS tag of $T$.
\item Index($T$).
\item Conjoined PoS tag of $T$ and Index($T$).
\item The depth of $T$ in the syntactic dependency parse. 
\end{itemize}

\subsubsection{Logistic Regression}


\section{Features}

\subsection{all the stuff in our model}

\label{s:features}
These features were computed over an edge $E$ with source token $S$ and target token $T$.  For each of those listed here, we have an indicator feature for each value it takes on. 

\begin{itemize}
\item BasicFeatures
\begin{itemize}
\item Lemmas of $S$ and $T$.
\item Part of speech tags of $S$ and $T$.
\item Token strings of $S$ and $T$.
\item 1 if Index($S$) $\le$ Index($T$), 0 otherwise.
\end{itemize}
\item LinearOrderFeatures
\begin{itemize}
\item Index($S$) - Index($T$).
\end{itemize}
\item CoarseDependencyFeatures
\begin{itemize}
\item 1 if $S$ is the parent of $T$ in syntactic dependency parse, 0 otherwise.
\item Distance between $S$ and $T$ in syntactic dependency parse.
\end{itemize}
\item LinearContextFeatures
\begin{itemize}
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($S$)+1, Index($T$)-1, Index($T$), Index($T$)+1.
\item Concatenated PoS tags of tokens at Index($S$)-1, Index($S$), Index($T$)-1, Index($T$).
\item Concatenated PoS tags of tokens at Index($S$), Index($S$)+1, Index($T$), Index($T$)+1.
\end{itemize}
\item DependencyPathv1
\begin{itemize}
\item The token string concatenated to the labeled path through the syntactic dependency tree from $S$ to $T$.
\end{itemize}
\item SubcatSequenceFE
\begin{itemize}
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the label of the ark to the child. If $c$ is $T$, append a ``+''.
\item For each child $c$ of $S$ in the syntactic dependency tree, concatenate the PoS tag of $S$ with the PoS tag of $c$ and the label of the ark to the child. If $c$ is $T$, append a ``+''.
\end{itemize}
\item UnlabeledDepFE
\begin{itemize}
\item The unlabeled path through the syntactic dependency tree from $S$ to $S$. 
\item The unlabeled path through the syntactic dependency tree from $S$ to $T$, annotated with whether the each step through the tree was to the right or left in the sentence.
\end{itemize}
\item Brown clusters
\begin{itemize}
\item \jdcomment{The word on the street is that these weren't used. Check on that, then consider moving to ``didn't work'' section.}
\end{itemize}
\end{itemize}
\subsection{feature hashing}



\subsection{all the stuff we tried but didn't work}
\subsubsection{Word vectors}
Word vectors have been shown to benefit a number of semantic tasks. We incorporated features based on trained word vectors in a number of ways, though we didn't find benefit from any of them. The vectors we used \cite{wordVectors} were trained trained on the WMT-2011 dataset, using a variation on latent semantic analysis which incorporated information from multiple languages. The vectors themselves contained 64 dimensions. 

 For our edge factored model \logitedge, when considering a potential edge, we had a source word $W_1$ with associated word vector $V_1$ and a target word $W_2$ with associated word vector $V_2$. For each method outlined below, the elements of the output vector was used as a feature. As an example, if we concatenated the two vectors into a third vector $V_3$ (as we show in $f_1$), each element of $V_3$ was used as a feature in our model. 
\begin{enumerate}
\item $f_1(V_1,V_2)$ = $ \left( \begin{smallmatrix} V_1\\ V_2 \end{smallmatrix} \right)$
\item $f_2(V_1,V_2)$ = $V_1 - V_2$.
\item $f_3(V_1,V_2)$ = $V_1 \cdot V_2$ (Dot product)
\item $f_4(V_1,V_2)$ = $V_1 \bigodot V_2$ (Element-wise multiplication)
\end{enumerate}
Unfortunately we saw no improvement from these features.

\subsubsection{Random Forests}
The word vectors added as features didn't reliably improve accuracy on any of the formalisms. One hypothesis as to why was that the vectors had a non-linear relationship with the labels. To address this, we trained a random forest in R\footnote{http://cran.r-project.org/web/packages/randomForest/randomForest.pdf} with the word vector features as described above, and used the hard labels predicted by the random forests as features in the \logitedge\ parser. Again, we saw no improvement.

\bocomment{caveat, these were not carefully done.}


\section{Evaluation}
\label{s:datasplits}

\section{Future work}

\section{Conclusion}

\nocite{flanigan-etal:ACL2014}



\section*{Acknowledgements}

\bibliographystyle{acl}
\bibliography{semeval8,morebib}




\end{document}
