%
% File twocolumn.tex
%
%%
%% Based on the style files for *SEM-2014, which were, in turn,
%% Based on the style files for COLING-2014, which were, in turn,
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{semeval2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}
\usepackage{color}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{framed}
\usepackage{subcaption}

\setitemize{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}
\setenumerate{noitemsep,topsep=10pt,parsep=0pt,partopsep=0pt}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand{\wsname}{SemEval-2014}
\newcommand{\submissionpage}{\url{http://alt.qcri.org/semeval2014/index.php?id=cfp}}
\newcommand{\filename}{semeval2014}
\newcommand{\contact}{pnakov qf.org.qa}


\definecolor{myblue}{rgb}{0,0.1,0.6}
\definecolor{mygreen}{rgb}{0,0.3,0.1}
\usepackage[colorlinks=true,linkcolor=black,citecolor=mygreen,urlcolor=myblue]{hyperref}
\newcommand{\transpose}{^\mathsf{T}}
\DeclareMathOperator*{\argmax}{arg\,max}

\newcommand{\bocomment}[1]{\textcolor{Bittersweet}{[#1 -BTO]}}
\newcommand{\sam}[1]{\textcolor{blue}{[#1 -SMT]}}
\newcommand{\nas}[1]{\textcolor{red}{[#1 -NAS]}}
\newcommand{\jmf}[1]{\textcolor{orange}{[#1 -JMF]}}
\newcommand{\jdcomment}[1]{\textcolor{NavyBlue}{[#1 -JDD]}}

% \newcommand{\bocomment}[1]{}
% \newcommand{\sam}[1]{}
% \newcommand{\nas}[1]{}
% \newcommand{\jmf}[1]{}
% \newcommand{\jdcomment}[1]{}

% turn off for submission, but on for a more tech report-y version.
% \newcommand{\codenote}[1]{\textcolor{PineGreen}{[#1]}}
\newcommand{\codenote}[1]{}
\newcommand{\logitedge}{\textsc{LogitEdge}}
\newcommand{\noedge}{\textsc{NoEdge}}

% saves only a little bit of space. might need tweaking to save more.
\newenvironment{itemizesquish}{\begin{list}{\labelitemi}{\setlength{\itemsep}{-0.1em}\setlength{\labelwidth}{1em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}



\title{CMU-ARK: Arc-Factored, Discriminative Semantic Dependency Parsing}

\author{
	Sam Thomson \quad
	Brendan O'Connor \quad
	Jeff Flanigan \quad
	David Bamman \quad  \\
	\bf{Jesse Dodge \quad
	Swabha Swayamdipta \quad
	Nathan Schneider \quad
	Chris Dyer \quad
	Noah A.~Smith} \\
  Language Technologies Institute \\
  Carnegie Mellon University \\
  Pittsburgh, PA 15213, USA \\
  {\tt\{sthomson,brenocon,jflanigan,dbamman,jessed,}\\
   \tt{swabha,nschneid,cdyer,nasmith\}@cs.cmu.edu}
}

\date{}

\begin{document}
\maketitle

\begin{abstract}
We present an arc-factored statistical model for broad-coverage
semantic dependency parsing, as defined by the SemEval 2014 Shared
Task 8 on
Broad-Coverage Semantic Dependency Parsing.    Our entry in the open
track placed second in the competition.
\end{abstract}



\section{Introduction}

The task of broad coverage semantic dependency parsing aims to provide a
shallow semantic analysis of text not limited to a specific domain.
As distinct from deeper semantic analysis (e.g., parsing to a full
lambda-calculus logical form), shallow semantic parsing captures relationships
between pairs of words or concepts in a sentence, and has wide application for
information extraction, knowledge base population, and question answering (among others).
SemEval 2014 Shared Task 8 on Broad-Coverage Semantic Dependency
Parsing \cite{oepens_broad_2014} includes three different semantic
formalisms (\S\ref{s:formalisms}), which encourages finding features and methods
that are broadly applicable to semantic parsing, and discourages optimization
toward a single semantic representation.
%By diversifying the representations, we better tackle the broader task of
%semantic analysis itself.

We present here two systems that produce semantic dependency parses by extracting
features for each potential dependency arc and learning a statistical model to
discriminate between good arcs and bad;
the first treats each labeled edge decision as an independent multiclass
logistic regression (\S\ref{s:logitedge}), while the second predicts arcs
as part of a graph-based structured support vector machine (\S~\ref{s:graphparser}).
Common to both models are a rich set of features on arcs, described in
\S\ref{s:edgefeatures}.  We include a discussion of features found to
have no discernable effect, or negative effect, during development (\S\ref{s:badfeatures}).

Our system placed second in the open track of the broad-coverage semantic
parsing task (in which output from syntax parsers and other outside resources \emph{can} be used).
We present our results in \S\ref{s:evaluation}.



\section{Formalisms} \label{s:formalisms}

The organizers of Shared Task 8 provided a corpus of English news text
drawn from the Penn Treebank, annotated according to three different semantic dependency formalisms.
\emph{DM} is automatically derived from LinGO English Resource Grammar (ERG)
annotations in DeepBank \cite{flickinger_deepbank_2012}.
\emph{PAS} is automatically derived from the Enju HPSG treebank using the
conversion rules of \newcite{miyao_corpus_oriented_2004}.
\emph{PCEDT} is automatically derived from the tectogrammatic layer of the
Prague Czech-English Dependency Treebank \cite{hajic_building_1998}.
See Figure~\ref{fig:formalisms} for an example sentence.
\begin{figure*}
	\centering
		\includegraphics[width=\textwidth]{fig/example_dm} \\
		\includegraphics[width=\textwidth]{fig/example_pas} \\
		\includegraphics[width=\textwidth]{fig/example_pcedt}
	\caption{Example annotations for \emph{DM} (top), \emph{PAS}
          (middle), and \emph{PCEDT} (bottom). \jmf{are the
    three LOC's in PCEDT correct? is the capitalization of 'BAKER
    FALTERS' meaningful?}}
	\label{fig:formalisms}
\end{figure*}

The three formalisms come from very different linguistic theories, but all
three are represented as labeled directed graphs, with words as vertices, and
all three have ``top'' annotations, corresponding roughly to the
semantic focus of the sentence.  The ``top'' need not be a root of the
graph.
This allows us to use the same machinery (\S\ref{s:models}) to simultaneously
train and test statistical models for the three formalisms.
\sam{Stats about \% multiple roots, \% multiple tops, \% tree, \% acyclic}



\section{Models} \label{s:models}

We treat the problem as a three-stage pipeline.
The first stage prunes words by predicting whether has any incoming or
outgoing edges at all (\S\ref{s:singleton_model}); if not, then it need not be 
considered in later stages.
The second stage jointly predicts whether an edge is
present, and if so, its label (\S\ref{s:edge_model}).
The third stage predicts whether a word is a \emph{top} or not
(\S\ref{s:top_model}).
Formalisms sometimes annotate more than one ``top'' per sentence, but we
found that we achieve the best performance on all formalisms by predicting only
the one best-scoring ``top'' under the model.
\bocomment{I thought that only LogitEdge has the separate third stage of ``top'' prediction. Maybe I'm wrong? delete this comment if so}
\bocomment{Also, singleton pruning does not matter for LogitEdge.  It doesn't affect accuracy, I'm pretty sure.  But it is essential for the graph model.}


\subsection{Singleton Classification} \label{s:singleton_model}

We call a node in a graph without any parents or children a \emph{singleton}.
For singleton prediction, we train a simple token-level logistic regression
classifier.
We train a model for each of the three formalisms, as each of the three
formalisms follow different conventions determining whether a token is
included in the final graph for a sentence.
\emph{PAS}, for example, includes almost all tokens that are not
punctuation or a determiner, and our singleton model achieves over 99\%
accuracy.
\jdcomment{I need to find the accuracy for the other two formalisms. Will
include this in our Results section.}

For each token $t$, the features include
the token $t$, its lemma, and its part-of-speech tag.



\subsection{Edge Prediction} \label{s:edge_model}

In the second stage of the pipeline, we predict the set of labeled directed
edges in the graph.
We use the same set of edge-factored features (\S\ref{s:features}) in two
different models.
For the \emph{DM} and \emph{PCEDT} formalisms, we use an edge-independent multiclass logistic
regression model (\logitedge, \S\ref{s:logitedge}).
For the \emph{PAS} formalism, we use a structured SVM 
\cite{taskar_max_2003,tsochantaridis_support_2004} that enforces 
\emph{determinism}\jmf{I'll find a citation} constraints that allow a word to
have at most one outgoing edge with the
same label (\S\ref{s:graphparser}).
\sam{For each formalism, we trained both models with varying features enabled
and hyperparameter settings and submitted the configuration that produced the
best labeled $F_1$ on the development set.
We report results only for the submitted configurations.
Due to time constraints, full feature and hyperparameter sweeps were not
possible.
Jeff, please make it clear here about what happened in the 11th
hour.}
%\sam{explain why we use different models for different formalisms}
%\bocomment{shouldn't we just say, we tried both for all and are reporting the
% configurations that performed best on devset?}
%\sam{yes, that sounds good. waiting on graph vs. non-graph results from jflan}


\subsubsection{\logitedge\ Parser\jmf{I think we should call this
\sc{LogisticEdge}}}
\label{s:logitedge}


\codenote{LRParser.java}

The \logitedge\ model considers only token index pairs $(i, j)$ where %that are
% within 10 tokens of each other (i.e.~
$|i-j| \leq 10$, $i \ne j$, and both $t_i$ and
$t_j$ have been predicted to be non-singletons by the first stage.
\jmf{actually, the graph parser doesn't have the distance constraint}
\sam{this could account for the discrepency in performance btwn logit/svm. we
found that this pruning helped.}
% In other words, edges with more than 9 tokens between its two endpoints are not
% considered.
Although this prunes some gold edges, among the formalisms,
95\%-97\% of all gold edges are between tokens of distance 10 or less.
We found that pruning long edges increased accuracy overall, in addition to
speeding up training and prediction.
Both directions $i \rightarrow j$ and $j \rightarrow
i$ are considered between every pair.

%\noindent
The model treats every ordered pair of token indices $(i, j)$ as a
multiclass logistic regression with $K+1$ possible outputs:
the formalism's original $K$ edge labels plus the additional label \noedge,
which indicates that no edge exists from $i$ to $j$.
Let $L$ be the set of possible edge labels.

% In our implementation, we separate features into two classes:
% those which can be extracted for any one token, $\bm{f}^{(\text{node})}(i)$;
% and those which look at both the parent and the child,
% $\bm{f}^{(\text{edge})}(i, j)$.
% These features are described in \S\ref{s:features}.
% Then the final feature vector $\bm{f}(i, j)$ is $\bm{f}^{(\text{edge})}(i,
% j)$, along with the $\bm{f}^{(\text{node})}$ features for both the parent and
% child token.
% The final feature vector is made by conjoining the output label with
% each:
% \[
% 	\bm{f}(i, j, \ell) := \text{conjoin}(\ell, \bm{f}^{\text{percept}}(i, j))
% \]
% where
% 
% \begin{aligned}
% \bm{f}^{\text{percept}}(i, j) := & \bm{f}^{(\text{edge})}(i, j) \\
% 	& \cup \text{ conjoin}(\text{``par''}, \bm{f}^{(\text{node})}(i)) \\
% 	& \cup \text{ conjoin}(\text{``child''}, \bm{f}^{(\text{node})}(j))
% \end{aligned}
% \[
% \text{conjoin}(\alpha, M) := \{ ((\alpha, k), v) \mid (k, v) \in M \}
% \]
%Feature extractors are defined to extract
%features both for individual tokens ($f^{(\text{node})}(i)$), and also for
% pairs of tokens ($f^{(\text{edge})}(i,j)$), which are conjoined against all possible output
%labels.
% \noindent
For candidate parent index $i$ and child index $j$, we extract a feature vector
$\bm{f}(i, j)$, as described in \S\ref{s:features}.
The multiclass logistic regression model defines a distribution over $L$,
parametrized by the weight vectors $\Phi = \{\bm\phi_\ell\}_{\ell \in L}$:
\[
  P_\Phi(\ell)  = \frac{
  	\exp\{\bm\phi_\ell \cdot \bm{f}(i, j)\}
  } {
  	\sum_{\ell^\prime \in L} {
  		\exp\{\bm\phi_{\ell^\prime} \cdot \bm{f}(i, j)\}
  	}
  }
\]

\noindent
%In this notation, an $f$ function outputs a real-valued vector, defined as the
% concatenation of features from all the feature classes (\S\ref{s:features}).
$\Phi$ is learned by minimizing total negative log-likelihood of the above
(with weighting; see below), plus $\ell_2$ regularization.
Adagrad \cite{duchi_adaptive_2011} is used for optimization.
This seemed to optimize faster than L-BFGS \cite{byrd_limited_1995}, at least for earlier
iterations, though we did no systematic comparison. Stochastic gradient steps are applied one at a time from individual examples, and a gradient step for the regularizer is applied once per epoch.

There is a class imbalance in the output labels; in all three formalisms, there
are many more $\noedge$ examples than true edge examples.
\sam{If we get these numbers, it would be nicer to quantify, and say something
like ``In the FFF formalism, the training set (\S\ref{s:evaluation}) contains NNN
candidate edges (pairs of tokens with length between 1 and 10 inclusive), and NNN actual (non-null) edges.''}
We found that $F_1$ performance was improved by
downweighting $\noedge$ examples through a weighted log-likelihood objective,
$\sum_{i,j} \sum_\ell w_\ell \log P(y_{i,j}=\ell; \bm\phi)$, with $w_{\noedge}=0.3$
and $w_{\ell} = 1, \forall \ell \in L$.
(The weight was chosen by gridsearch on a development set.)

% 0.4

% Besides the edge logistic regression system, there were both pre- and post-processing steps.

% \textbf{Preprocessing:}
% \bocomment{TODO need to check how much preproc was used for this.  Was singleton pruning turned on?  It looks like we commented out the prune features in LRParser.java.  But singleton pruning might have been turned on.}

% \textbf{Decoding and postprocessing:}
\textbf{Decoding:}
\codenote{MyGraph::decodeEdgeProbsToGraph()}

\bocomment{may15 11:23am: The below describes how it works in LogitEdge.  Jeff, please confirm this is how you use the topness classifier, or modify appropriately.}

To predict a graph structure at test-time for a new sentence,
the most likely edge label is predicted for every candidate $(i, j)$ pair of
tokens that has not been pruned by an earlier stage.
We enforce only one graph constraint, which is that there cannot be
an edge in both directions between any pair of words.
If an edge is predicted for both directions for a single $(i, j)$
pair, only the edge with the higher score is chosen.
There are no such bidirectional edges in the training data.
This enforcement actually did not improve accuracy on \emph{DM} or \emph{PCEDT};
it did improve \emph{PAS} by $\sim 0.2\%$ absolute $F_1$-score, but we did not submit \logitedge\ for \emph{PAS}.
\codenote{\url{https://github.com/Noahs-ARK/semeval-2014/pull/21}}


\subsubsection{Structured SVM Parser with Determinism Constraints} \label{s:graphparser}

\nas{note section title change above; ``Deterministic Structured SVM''
  is easily misparsed to suggest a very different meaning!}


To predict edges for the \emph{PAS} formalism, we use a model with determinism
constraints.  This ensures that each word token has at most one
outgoing edge for each label type.
\bocomment{need to answer here: why only PAS?}

Consider the fully dense graph of all edges between all words predicted
as not singletons by the singleton classifier \S\ref{s:singleton_model} (in all
directions with all possible labels).
If $\Psi = \{\bm\psi_\ell\}_{\ell \in L}$ denotes the model
weights, and $\bm{f}$ denotes the same features as the
\logitedge~model, then an edge from $i$ to $j$ with label $\ell$ in the
dense graph has a weight $c(i,j,\ell)$ assigned to it using the linear
scoring function: \nas{no-edge is not included here, I am guessing.
  say so explicitly since this is a subtle difference with the logit model}
\[
c(i,j,\ell) = \bm\psi_\ell \cdot \bm{f}(i,j)
\]
The decoder finds the highest scoring subgraph of the dense graph
subject to the determinism constraints.  Let $edge(i,j,\ell)$ be a
binary function where $edge(i,j,\ell) = 1 $ denotes inclusion of an edge
from $i$ to $j$ with a label $\ell$.  Then $edge(i,j,\ell)$ is computed
as follows:
\[
edge(i,j,\ell) =
\begin{cases}
1 & \text{if } j = \argmax_{k} c(i,k,\ell) \\
& \text{and } c(i,j,\ell) > 0 \\
0 & \text{otherwise}
\end{cases}
\]
In other words, for each node and each label, the decoder searches for
the highest scoring outgoing edge and adds it if its weight is
positive.  The algorithm's runtime is $O(n^2)$. \nas{I can't tell
  whether you are claiming this algorithm gives the optimal subgraph
  (according to some unstated criterion) or if it's just a hack.
  don't make me guess!  say how we are defining the best structure,
  and state explicitly that our algorithm exactly optimizes that
  criterion in quadratic time}

The model weights are trained using the structured SVM loss.  If $x$
is a sentence and $y$ is a graph over that sentence, let the features 
be denoted $\bm{f}(x,y) = \sum_{(i,j,\ell) \in edges(y)}
\bm{f}(i,j,\ell)$.  The SVM loss for each training example $(x_i, y_i)$ is:
\begin{multline*}
-\bm\psi^\top \bm{f}(x_i,y_i) + \max_{y} \bm\psi^\top \bm{f}(x_i,y) +
\mathit{cost}(y,y_i)
\end{multline*}
where $\mathit{cost}(y,y_i)$
is the cost function
\begin{multline*}
\mathit{cost}(y,y_i) = \alpha |edges(y)\setminus edges(y_i)| + \\
\beta |edges(y_i)\setminus edges(y)|.
\end{multline*}
$\alpha$ and $\beta$ trade off between precision and recall for the
edges.  The loss is minimized with AdaGrad
using early-stopping on a development set. % Tops are predicted using the
%top prediction model \S\ref{s:top_model}.


\subsubsection{Edge Features}
\label{s:edgefeatures}

\label{s:features}

\sam{would be nice to cite where we stole these features from, where applicable}
Table~\ref{table:edgefeatures} describes the features we used for predicting
edges.
These features were computed over an edge $e$ with parent token $s$ at
index $i$ and child token $t$ at index $j$. 
Unless otherwise stated, each feature template listed has an indicator
feature that fires for each value it can take on.
\begin{table}
\begin{framed}
\begin{small}
\textbf{Bias:} Always fires.

\textbf{Tokens:} The tokens $s$ and $t$ themselves.

\textbf{Lemmas:} Lemmas of $s$ and $t$.

\textbf{POS tags:} Part of speech tags of $s$ and $t$.

\textbf{Linear Order:} Fires if $i < j$.

\textbf{Linear Distance:} $i - j$.

\textbf{Dependency Path:} The concatenation of all tokens and arc labels
	on the labeled path in the syntactic dependency tree from $s$ to $t$.

\textbf{Up/Down Dependency Path:} The sequence of upward and
	downward moves needed to get from $s$ to $t$ in the syntactic dependency
	tree.

\textbf{Up/Down/Left/Right Dependency Path:} The unlabeled path through
	the syntactic dependency tree from $s$ to $t$, annotated with whether the each
	step through the tree was up or down, and whether it was to the right or left
	in the sentence.

\textbf{Is Parent:} Fires if  $s$ is the parent of $t$ in the syntactic
	dependency parse.

\textbf{Dependency Path Length:} Distance between $s$ and $t$ in the
	syntactic dependency parse.

\textbf{POS Context:} Concatenated POS tags of tokens at $i-1$, $i$,
	$i+1$, $j-1$, $j$, and $j+1$. Concatenated POS tags of tokens at $i-1, i, j-1$,
	and $j$. Concatenated POS tags of tokens at $i, i+1, j$, and $j+1$.

\textbf{Subcategorization Sequence:} 
	The sequence of dependency arc labels out of $s$, ordered by the index of the
	child.
	Distinguish left children from right children.
	If $t$ is a direct child of $s$, distinguish its arc label with a ``+''.
	Conjoin this sequence with the POS tag of $s$.

\textbf{Subcategorization Sequence with POS:} As above, but add the
	POS tag of each child to its arc label.
\end{small}
\end{framed}
\caption{Features used in edge prediction}
\label{table:edgefeatures}
\end{table}


\subsubsection{Feature Hashing}

%\bocomment{ALTERNATE VERSION TO SAVE SPACE:
  The biggest memory usage was in the hash table implementing the map
  of feature names to integer indexes, used during feature extraction.
  In order to facilitate more convenient experimentation, we
  implemented multitask feature hashing
  \cite{weinberger_feature_2009}, which randomly assigns features'
  numeric indexes with a hash function, under the theory that errors
  due to collisions tend to cancel each other out.  We found no
  drop in accuracy when using the technique.

% }
% \sam{I think we can get away with this short version}
% %% 7-10 million features (percepts) before label conjunction:
% % cab:~dbamman/semeval/mar3 % wc -l *.model
% %    7518732 mar3_recsplit.dm.model
% %    7038841 mar3_recsplit.pas.model
% %    9939549 mar3_recsplit.pcedt.model
% %   24497122 total

% \codenote{
% \url{https://github.com/Noahs-ARK/semeval-2014/pull/20}
% Code:
%   See use of flag LRParser.useHashing, esp LRParser::perceptNum() 
%   and Model::coefIdx(): hashed values before and after label conjunction.
% }

% \noindent
% For \logitedge, the biggest memory usage was from
% the hash table that implemented
% a map of feature names to integer indexes, used during feature extraction.
% Tens of millions of features (before conjunction) resulted in several gigabytes of memory use.  To facilitate more convenient experimentation,
% we eliminated this by using
% multitask feature hashing
% \cite{weinberger_feature_2009},
% which randomly assigns features' numeric indexes with a hash function.\footnote{We also tried replacing the hash table with a Patricia Tree, trying several open-source implementations; but they had worse memory usage than a GNU Trove hash table. 
%   \codenote{\url{https://github.com/brendano/myutil/blob/0a5697a7f066c265a75ce1b7bd527feafa683c8a/src/vocabalts/vocabalts_results.txt}}}
% Let $F$ be the space of all feature names (before conjunction with the output label).
% In the usual approach, a hash table is maintained containing a
% one-one onto map from features to integer indexes $g: F \rightarrow \{1,\ldots,|F|\}$,
% and each output label has an array of model parameters $\bm\phi^\ell \in \mathbb{R}^{|F|}$,
% which $g(f)$ indexes into.
% In the hashing approach, a hash function $h_{\ell}$
% maps feature name to one of $B$ bucket values,
% $h_{\ell} : F \rightarrow \{1, \ldots, B\}$.
% (A different hash function is used for each output label.)
% The model parameters are stored in an array $h_\ell(\bm\phi^\ell) \in \mathbb{R}^{B}$,
% for which an entry is defined as
% \[
%   h_\ell(\bm\phi^\ell)_{b} = 
%   \sum_{\substack{\text{feat} \in F 
%     \\\text{ s.t. } h_\ell(\text{feat})=b}}
%   {\phi^\ell_{\text{feat}}}
% \]
% This is easy to implement:
% array lookups and gradient steps are simply implemented with $h$ instead of $g$, and feature vectors are also stored in a hashed space.
% \cite{weinberger_feature_2009} show that, even with some collisions,
% dot products are approximately preserved with high probability;
% and indeed, we observed no drop in accuracy compared to using an explicit feature map.
% We used a value of $B$ as large as was convenient for memory usage (due only to $h(\phi)$), $B=10^{8}$.
% This is actually higher than $|F|\approx 10^7$, though plenty of collisions still happen: about 9.5\% of features are hashed to a value shared by at least one other feature.

% %% simulation:
% % num buckets count 0: 90484843
% % num buckets count 1: 9046266
% % num collisions: num features that are in a non-singleton bucket: 953734
% %%  average over simulations for last value: 9.518e+05 


\subsection{Top Prediction} \label{s:top_model}

We trained a separate token-level binary logistic regression to classify
whether a token's node had the ``top'' attribute or not.
At decoding time, all predicated predicates (i.e., nodes where there is at least one outbound edge)
are possible candidates to be ``top'';
the classifier probabilities are evaluated, and the highest-scoring node is
chosen to be ``top.''
\codenote{MyGraph::decideTops()}
This is suboptimal, since some graphs have multiple tops (in \emph{PCEDT} this is
more common);
but selection rules based on probability thresholds gave worse $F_1$
performance on the development set. \codenote{\url{https://github.com/Noahs-ARK/semeval-2014/issues/37}}

For a given token $t$ at index $i$, the top classifier's features
included $t$'s POS tag, $i$, $i$, those two conjoined, and the depth
of $t$ in the syntactic dependency tree.


\section{Negative Results}

\subsection{Features}
\label{s:badfeatures}

We followed a forward-selection process during feature engineering.
For each potential feature, we tested the current feature set versus the current
feature set plus the new potential feature.
If the new feature did not improve performance, we did not add it.
We list below some of the features which we tested, but did not find to improve
performance.

In order to save time, we ran these feature selection experiments
on a subsample of the training data, for a reduced number of iterations.
The following results thus have a 
the strong caveat that the experiments were
not exhaustive.  It could be these that any of the following features would help under more careful testing.

\nas{NOAH LEFT OFF HERE}

\subsubsection{Word vectors}

We experimented with features derived from distributional word embeddings.
The vectors we used \cite{wordVectors} were trained trained on the WMT-2011
dataset, using a variation on latent semantic analysis which incorporated
information from multiple languages.
The vectors contained 64 dimensions.

%For our edge factored model \logitedge, w
When considering a potential edge, we
had a parent token $s$ with associated word vector $\bm{v_1}$ and a child token
$t$ with associated word vector $\bm{v_2}$.
For each method outlined below, each dimension of the output vector was used as
a feature, whose value was the value of the vector along that dimension.
%As an example, if we concatenated the two vectors into a third vector $V_3$ (as
%we show in $f_1$), each element of $V_3$ was used as a feature in our model.

\begin{itemize}
\item Concatenation:
$ \left( \begin{smallmatrix} \bm{v_1}\\ \bm{v_2}
\end{smallmatrix} \right)$
\item Difference: $\bm{v_1} - \bm{v_2}$.
\item Dot product: $\bm{v_1} \cdot \bm{v_2}$
\item Element-wise multiplication: $\bm{v_1} \bigodot \bm{v_2}$
\end{itemize}
Unfortunately we saw no improvement from these features.

\subsubsection{Random Forests Over Word Vectors}
The word vectors added as features didn't reliably improve accuracy on any of
the formalisms.
One hypothesis as to why was that the vectors had a non-linear relationship
with the labels.
To address this, we trained a random forest in
R\footnote{\url{http://cran.r-project.org/web/packages/randomForest/randomForest.pdf}}
with the word vector features as described above, and used the hard labels
predicted by the random forests as features in the \logitedge\ parser.
Again, we saw no improvement.

\subsubsection{Active vs. Passive Voice}
A verb tends to realize its semantic arguments differently when the verb is in
active voice vs. passive voice.
We attempted to capture this by conjoining a ACTIVE/PASSIVE feature with both
the LinearDistance features and the SubcatSequence features.
\bocomment{From reading this i don't understand how this was computed. word patterns for "has been" etc? PTB POSes? left-vs-right directionality?}

There was no improvement from including these features.
Since we were already using dependency features from the 
organizers' supplied Stanford Dependencies-style parses,
which include passivization information in arc labels (\emph{nsubjpass},\emph{auxpass}),
we hypothesize this information may have already been incorporated into the syntactic analysis preprocessing.

\subsubsection{Alternative Distance Measures}
We tried multiple kinds of distance features, including log distance, binned
distance (i.e. less than five tokens apart, between five and ten tokens apart, more than
ten tokens apart), and thresholded distance (i.e. zero to three tokens apart,
zero to five tokens apart, etc.).

\subsubsection{Brown Clusters}
We experimented with Brown cluster features \cite{Brown:1992:CNG:176313.176316}
for tokens in the sentence.
These clusters were obtained by training on a large corpus of data crawled from
the web.
We used all cluster prefixes of length 2, 4, 6, 8, 10 and 12.
We added a padding when the length of cluster was less than the required prefix
size.
We added the Brown cluster for the parent token, the child token, and both
conjoined.
No combination of the above settings helped performance.
Neither did conjoining the above features with the POS tags of the parent and
child tokens.

\subsection{Graph Constraints}
\label{s:badconstraints}

We experimented with enforcing other graph constraints in the structured SVM
decoder (\S\ref{s:graphparser}).
We tried enforcing that the graph was connected (ignoring singletons), similarly
to \newcite{flanigan-etal:ACL2014}.
Almost all semantic dependency graphs are connected (ignoring singletons) in the
training data, but we found that enforcing this constraint significantly hurt
performance.
Enforcing this constraint causes more edges to be predicted, and we conjecture
that because the evaluation metric penalizes wrongly labeled edges more than
false negatives, 
\bocomment{huh? don't they count equally?}
this hurts overall.

We also tried enforcing a tree constraint, although most gold graphs are not
trees.
Unsurprisingly, we found that enforcing a tree constraint hurt performance.



\section{Experiment Setup}
\label{s:evaluation}

We participated in the Open Track, and used the syntactic dependency parses supplied by the organizers.  Feature engineering was performed on a development set (\S 20), training on \S 00--19.

%\section{Evaluation}

We evaluate labeled precision (LP), labeled recall (LR), labeled $F_1$ (LF), and
labeled whole-sentence match (LM) on the held out test data using the
evaluation script provided by the organizers (see Table~\ref{table:perf}).
LF was averaged over the formalisms to determine the winning system.

% CMU:
% DM:
% LP: 0.844641
% LR: 0.834849
% LF: 0.839716
% LM: 0.087537
% 
% 
% PAS:
% LP: 0.907832
% LR: 0.885141
% LF: 0.896343
% LM: 0.260386
% 
% 
% PCEDT:
% LP: 0.768139
% LR: 0.707173
% LF: 0.736396
% LM: 0.071217

% Priberam:
% DM:
% LP: 0.902322
% LR: 0.881050
% LF: 0.891559
% LM: 0.268546
% 
% PAS:
% LP: 0.925576
% LR: 0.909676
% LF: 0.917557
% LM: 0.378338
% 
% PCEDT:
% LP: 0.801397
% LR: 0.757900
% LF: 0.779042
% LM: 0.106825


\begin{table}
\begin{center}
\begin{tabular}%{\textwidth}%{.6\textwidth}
{@{\extracolsep{\fill}}c|cccc}%|c|c|c|c|c|c|}
% \hline
& \textbf{LP} & \textbf{LR} & \textbf{LF} & \textbf{LM} \\
\hline
\hline
\textbf{DM}
& 0.8446 & 0.8348 & 0.8397 & 0.0875 \\
\hline
\textbf{PAS}
& 0.9078 & 0.8851 & 0.8963 & 0.2604 \\
\hline
\textbf{PCEDT}
& 0.7681 & 0.7072 & 0.7364 & 0.0712 \\
\hline
\hline
\textbf{Average}
& 0.8402 & 0.8090 & 0.8241 & 0.1397 \\
\end{tabular}
\caption{Labeled precision (LP), recall (LR), $F_1$ (LF), and
whole-sentence match (LM) on the held out test data.
}
\label{table:perf}
\end{center}
\end{table}



\section{Future work}

There are a number of clear extensions to this work that could improve
performance.
While using an edge factored model allows for efficient inference, there is
much to be gained from using higher-order features.
Extending the model in this way often requires resorting to approximate
inference, but others have shown gains without a significant decrease in
speed \cite{mcdonald_online_2006,martins_turning_2013}.

While each of the three formalisms is different, there is much information
shared between them.
For example, the head of a sentence is often the same in all three formalisms.
Taking advantage of this shared structure in a multi-task learning framework
could lead to gains.

There is additional structure in the formalisms which were unable to take
advantage of due to time constraints.
PCEDT annotations were originally a tree, but underwent a preprocessing step by
the task organizers in order to better capture bilexical dependency
relationships.
Wherever there is a coordination arc\footnote{A coordination arc
is one of ``CONJ.member'', ``APPS.member'', ``GRAD.member'', ``CONTRA.member'',
or ``REAS.member''.} present from $i \rightarrow j$, edges incoming to $i$ were
copied to instead point to $j$.
The ``LOC'' arcs incoming to ``paralysis'' and ``politics'' in
Figure~\ref{fig:formalisms}) are a result of this step, for example.
There was originally just one arc pointing to ``and''.
PCEDT trees were also mostly deterministic with respect to non-coordination arcs
before this step.
We conjecture that this copying is one reason that determinism constraints were
not helpful for PCEDT.
Formulating more subtle graph constraints that are able to capture this a priori
knowledge of the graph structure could lead to improved performance.


\section{Conclusion}
We found that feature-rich discriminative models performed well on the task of
mapping from sentences to semantic dependency parses.
Our work has shown the best results when using one model for predicting whether
a given token was to be included in the final graph, then using either our
\logitedge\ parser or a graph-based structured SVM parser to predict a graph
from the remaining tokens.
Each of the three formalisms has different constraints;
we found enforcing determinism in the PAS formalisms gave improvements, while
enforcing other constraints did not.
While our final approach was fairly standard for work in parsing, we tried a
number of additional features and constraints which did not help, and include
those negative results.



\section*{Acknowledgements}
The research reported in this article was sponsored by the U.~S.~Army Research
Laboratory and the U.~S.~Army Research Office under contract/grant number
W911NF-10-1-0533, DARPA grant FA8750-12-2-0342 funded under the DEFT program, US
NSF grants IIS-1251131\sam{, and \ldots}


\bibliographystyle{acl}
\bibliography{semeval8}




\end{document}
