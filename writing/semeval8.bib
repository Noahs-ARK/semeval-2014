%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for David Bamman at 2014-04-17 09:30:09 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@inproceedings{flanigan-etal:ACL2014,
	Address = {Baltimore, Maryland},
	Author = {J. Flanigan and S. Thomson and J. Carbonell and C. Dyer and N. A. Smith},
	Booktitle = {Proc. of ACL},
	Date-Added = {2014-04-17 13:29:45 +0000},
	Date-Modified = {2014-04-17 13:29:45 +0000},
	Month = {June},
	Publisher = {Association for Computational Linguistics},
	Title = {A Discriminative Graph-Based Parser for the Abstract Meaning Representation},
	Year = {2014}}


@inproceedings{weinberger_feature_2009,
	location = {New York, {NY}, {USA}},
	title = {Feature Hashing for Large Scale Multitask Learning},
	isbn = {978-1-60558-516-1},
	url = {http://doi.acm.org/10.1145/1553374.1553516},
	doi = {10.1145/1553374.1553516},
	series = {{ICML} '09},
	abstract = {Empirical evidence suggests that hashing is an effective strategy for dimensionality reduction and practical nonparametric estimation. In this paper we provide exponential tail bounds for feature hashing and show that the interaction between random subspaces is negligible with high probability. We demonstrate the feasibility of this approach with experimental results for a new use case --- multitask learning with hundreds of thousands of tasks.},
	pages = {1113–1120},
	booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	publisher = {{ACM}},
	author = {Weinberger, Kilian and Dasgupta, Anirban and Langford, John and Smola, Alex and Attenberg, Josh},
	year = {2009},
}


@article{duchi_adaptive_2011,
	title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
	volume = {12},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	pages = {2121–2159},
	journal = {J. Mach. Learn. Res.},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	urldate = {2013-09-16},
	date = {2011-07},
	year = {2011}
}